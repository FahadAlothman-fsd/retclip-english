{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-000",
   "metadata": {},
   "source": [
    "# ODIR-5K RET-CLIP Unified Pipeline\n",
    "\n",
    "This notebook implements a complete end-to-end pipeline for training **RET-CLIP** on the **ODIR-5K dataset** with English clinical text.\n",
    "\n",
    "---\n",
    "\n",
    "## Research Contribution\n",
    "\n",
    "**\"English BERT Embeddings for Binocular Retinal Image-Text Alignment\"**\n",
    "\n",
    "- ‚úÖ First validation of RET-CLIP architecture on English clinical text\n",
    "- ‚úÖ Cross-lingual transfer validation (original: Chinese ‚Üí our work: English)\n",
    "- ‚úÖ Real binocular fundus images from ODIR-5K (not duplicated monocular)\n",
    "- ‚úÖ Comparison of medical domain-specific vs general BERT models\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset: ODIR-5K (Ocular Disease Intelligent Recognition)\n",
    "\n",
    "- **5,000 patients** with genuine paired left/right eye fundus images\n",
    "- **10,000 images** total (2 per patient)\n",
    "- **Metadata**: Patient Age, Sex, Diagnostic Keywords (English)\n",
    "- **8 Disease Categories**: Normal, Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, Other\n",
    "\n",
    "---\n",
    "\n",
    "## RET-CLIP Architecture\n",
    "\n",
    "**Binocular Vision-Language Foundation Model**\n",
    "\n",
    "```\n",
    "Left Eye Image  ‚îÄ‚îÄ‚Üí  Vision Encoder  ‚îÄ‚îÄ‚Üí  Left Projection  ‚îÄ‚îê\n",
    "                                                             ‚îú‚îÄ‚Üí Tripartite Loss\n",
    "Right Eye Image ‚îÄ‚îÄ‚Üí  Vision Encoder  ‚îÄ‚îÄ‚Üí  Right Projection ‚îÄ‚î§\n",
    "                                                             ‚îÇ\n",
    "Clinical Text   ‚îÄ‚îÄ‚Üí  Text Encoder    ‚îÄ‚îÄ‚Üí  Text Embedding   ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Three-Level Contrastive Learning**:\n",
    "1. Left eye ‚Üî Left-specific clinical description\n",
    "2. Right eye ‚Üî Right-specific clinical description\n",
    "3. Patient-level ‚Üî Holistic diagnostic impression (both eyes)\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "1. Setup & Configuration        ‚Üí Install packages, authenticate APIs\n",
    "2. Load ODIR-5K Dataset         ‚Üí CSV metadata + paired fundus images\n",
    "3. Generate Clinical Prompts    ‚Üí DSPy + OpenRouter (3 prompts/patient)\n",
    "4. Preprocess for RET-CLIP      ‚Üí TSV + JSONL with eye_side annotations\n",
    "5. Build LMDB Database          ‚Üí Efficient PyTorch DataLoader format\n",
    "6. Train RET-CLIP               ‚Üí 10 epochs contrastive learning\n",
    "7. Zero-Shot Evaluation         ‚Üí Vision-language alignment test\n",
    "8. Linear Probing Evaluation    ‚Üí Feature quality assessment\n",
    "9. Final Report                 ‚Üí Metrics, comparison, artifacts\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Estimated Runtime\n",
    "\n",
    "| Mode | Patients | Prompts Time | Training Time | Total |\n",
    "|------|----------|--------------|---------------|-------|\n",
    "| TEST | 100 | ~30 min | ~30 min (2 epochs) | **~2-3 hours** |\n",
    "| FULL | 5,000 | ~4-5 hours | ~12-15 hours (10 epochs) | **~18-24 hours** |\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Google Colab** with A100 GPU (or T4 for testing)\n",
    "2. **API Keys**:\n",
    "   - HuggingFace Token: https://huggingface.co/settings/tokens\n",
    "   - OpenRouter API Key: https://openrouter.ai/keys\n",
    "3. **ODIR-5K Dataset**: Will be downloaded automatically\n",
    "\n",
    "---\n",
    "\n",
    "**Let's begin!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-001",
   "metadata": {},
   "source": [
    "# SECTION 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-002",
   "metadata": {},
   "source": [
    "## Cell 1.1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-004",
   "metadata": {},
   "source": [
    "## Cell 1.2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-006",
   "metadata": {},
   "source": [
    "## Cell 1.3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "\n",
    "# DSPy packages (for prompt generation)\n",
    "!pip install -q dspy-ai datasets huggingface-hub pandas pillow tqdm ipywidgets matplotlib\n",
    "\n",
    "# RET-CLIP packages (for training)\n",
    "!pip install -q ftfy regex\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "!pip install -q transformers\n",
    "!pip install -q lmdb\n",
    "!pip install -q scikit-learn seaborn\n",
    "\n",
    "# OpenCV for image processing\n",
    "!pip install -q opencv-python-headless\n",
    "\n",
    "# Kaggle dataset download (automatic!)\n",
    "!pip install -q kagglehub openpyxl\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-008",
   "metadata": {},
   "source": [
    "## Cell 1.4: Clone RET-CLIP Repository (with English BERT fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Clone repository with fixed RET-CLIP\n",
    "REPO_URL = \"https://github.com/FahadAlothman-fsd/retclip-english.git\"\n",
    "\n",
    "if not os.path.exists('/content/retclip_repo'):\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} /content/retclip_repo\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "# Copy retclip to /content/retclip\n",
    "if not os.path.exists('/content/retclip'):\n",
    "    print(\"\\nCopying fixed RET-CLIP...\")\n",
    "    shutil.copytree('/content/retclip_repo/retclip', '/content/retclip')\n",
    "    print(\"‚úÖ Copied to /content/retclip\")\n",
    "else:\n",
    "    print(\"‚úÖ /content/retclip already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, '/content/retclip')\n",
    "os.environ['PYTHONPATH'] = '/content/retclip'\n",
    "\n",
    "print(\"\\n‚úÖ RET-CLIP repository ready\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFY: Using fixed RET-CLIP with:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  1. ‚úì English BERT configs (PubMedBERT, BERT-base, BioBERT)\")\n",
    "print(\"  2. ‚úì URL-safe base64 encoding/decoding\")\n",
    "print(\"  3. ‚úì 3-column TSV format support (patient_id, left_img, right_img)\")\n",
    "print(\"  4. ‚úì DDP checkpoint loading with 'module.' prefix stripping\")\n",
    "print(f\"\\nLocation: /content/retclip\")\n",
    "print(f\"Source: {REPO_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-010",
   "metadata": {},
   "source": [
    "## Cell 1.5: Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "import os\n",
    "\n",
    "# TEST MODE: Set to True for quick testing with subset of data\n",
    "TEST_MODE = True  # Set to False for full dataset training\n",
    "NUM_TEST_PATIENTS = 100 if TEST_MODE else None\n",
    "\n",
    "# TRAINING HYPERPARAMETERS\n",
    "VISION_MODEL = \"ViT-B-16\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32 if TEST_MODE else 128\n",
    "NUM_EPOCHS = 2 if TEST_MODE else 10\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 500\n",
    "\n",
    "# TEXT ENCODER CONFIGURATION\n",
    "# Default text encoder (used when comparison is disabled)\n",
    "TEXT_MODEL = \"microsoft-BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "# TEXT ENCODER COMPARISON\n",
    "RUN_TEXT_ENCODER_COMPARISON = True  # Set to False to train only PubMedBERT (faster)\n",
    "\n",
    "# PROMPT GENERATION MODEL\n",
    "PRIMARY_MODEL = \"openrouter/google/gemini-2.5-flash-lite\"  # Fast, high-quality model for prompt generation\n",
    "\n",
    "# PROMPT GENERATION SETTINGS\n",
    "CHECKPOINT_INTERVAL = 10  # Save checkpoint every N patients\n",
    "DELAY_BETWEEN_CALLS = 0.5  # Delay between API calls (seconds) to avoid rate limiting\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Mode: {'TEST (100 patients, 2 epochs)' if TEST_MODE else 'FULL (5000 patients, 10 epochs)'}\")\n",
    "print(f\"  Vision Model: {VISION_MODEL}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Prompt Model: {PRIMARY_MODEL}\")\n",
    "print(f\"  Checkpoint Interval: {CHECKPOINT_INTERVAL} patients\")\n",
    "print(f\"  API Delay: {DELAY_BETWEEN_CALLS}s\")\n",
    "print(f\"\\nText Encoder Comparison: {'‚úÖ ENABLED' if RUN_TEXT_ENCODER_COMPARISON else '‚ùå DISABLED (PubMedBERT only)'}\")\n",
    "\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    print(\"   Will train and compare 3 text encoders:\")\n",
    "    print(\"   - PubMedBERT (medical domain)\")\n",
    "    print(\"   - BERT-base (general English)\")\n",
    "    print(\"   - BioBERT (biomedical domain)\")\n",
    "\n",
    "# Define text encoders for comparison\n",
    "# Note: model_id is for RET-CLIP config files (uses dashes), hf_model_id is for HuggingFace tokenizer (uses slashes)\n",
    "TEXT_ENCODERS = [\n",
    "    {\n",
    "        \"name\": \"PubMedBERT\",\n",
    "        \"model_id\": \"microsoft-BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"hf_model_id\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"description\": \"Medical domain-specific BERT trained on PubMed abstracts\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"BERT-base\",\n",
    "        \"model_id\": \"bert-base-uncased\",\n",
    "        \"hf_model_id\": \"bert-base-uncased\",\n",
    "        \"description\": \"General English BERT (baseline)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"BioBERT\",\n",
    "        \"model_id\": \"dmis-lab-biobert-base-cased-v1.1\",\n",
    "        \"hf_model_id\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
    "        \"description\": \"Biomedical domain BERT trained on PubMed + PMC\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# GOOGLE DRIVE PATHS (will be set after mounting)\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/RET-CLIP-ODIR\"\n",
    "DRIVE_DATA = f\"{DRIVE_BASE}/data\"\n",
    "DRIVE_PROMPTS = f\"{DRIVE_BASE}/prompts\"\n",
    "DRIVE_LMDB = f\"{DRIVE_BASE}/lmdb\"\n",
    "DRIVE_CHECKPOINTS = f\"{DRIVE_BASE}/checkpoints\"\n",
    "DRIVE_RESULTS = f\"{DRIVE_BASE}/results\"\n",
    "\n",
    "print(f\"\\nGoogle Drive paths configured:\")\n",
    "print(f\"  Base: {DRIVE_BASE}\")\n",
    "print(f\"  Data: {DRIVE_DATA}\")\n",
    "print(f\"  Prompts: {DRIVE_PROMPTS}\")\n",
    "print(f\"  LMDB: {DRIVE_LMDB}\")\n",
    "print(f\"  Checkpoints: {DRIVE_CHECKPOINTS}\")\n",
    "print(f\"  Results: {DRIVE_RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-012",
   "metadata": {},
   "source": [
    "## Cell 1.6: API Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Try to load from Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
    "    KAGGLE_USERNAME = userdata.get('KAGGLE_USERNAME')\n",
    "    KAGGLE_API_TOKEN = userdata.get('KAGGLE_API_TOKEN')\n",
    "    print(\"‚úÖ API keys loaded from Colab secrets\")\n",
    "except:\n",
    "    # Manual entry if secrets not available\n",
    "    HF_TOKEN = \"\"\n",
    "    OPENROUTER_API_KEY = \"\"\n",
    "    KAGGLE_USERNAME = \"\"\n",
    "    KAGGLE_API_TOKEN = \"\"\n",
    "    print(\"‚ö†Ô∏è Colab secrets not available - please set tokens manually\")\n",
    "\n",
    "# Authenticate with HuggingFace\n",
    "if HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Authenticated with HuggingFace\")\n",
    "else:\n",
    "    print(\"‚ùå HF_TOKEN not set\")\n",
    "\n",
    "# Configure Kaggle credentials\n",
    "if KAGGLE_API_TOKEN and KAGGLE_USERNAME:\n",
    "    os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "    os.environ['KAGGLE_KEY'] = KAGGLE_API_TOKEN\n",
    "    print(f\"‚úÖ Kaggle configured (username: {KAGGLE_USERNAME})\")\n",
    "elif KAGGLE_API_TOKEN:\n",
    "    os.environ['KAGGLE_KEY'] = KAGGLE_API_TOKEN\n",
    "    print(\"‚ö†Ô∏è Kaggle key set, but no username - may fail\")\n",
    "else:\n",
    "    print(\"‚ùå KAGGLE_API_TOKEN not set\")\n",
    "\n",
    "# Configure DSPy LLM with OpenRouter\n",
    "if OPENROUTER_API_KEY:\n",
    "    import dspy\n",
    "    \n",
    "    primary_lm = dspy.LM(\n",
    "        model=PRIMARY_MODEL,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        api_base=\"https://openrouter.ai/api/v1\",\n",
    "        extra_headers={\"HTTP-Referer\": \"https://chiron.app\", \"X-Title\": \"Chiron\"},\n",
    "        num_retries=3,\n",
    "    )\n",
    "    \n",
    "    dspy.configure(lm=primary_lm)\n",
    "    print(f\"‚úÖ LLM configured: {PRIMARY_MODEL.split('/')[-1]}\")\n",
    "else:\n",
    "    print(\"‚ùå OPENROUTER_API_KEY not set\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"API AUTHENTICATION STATUS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  HuggingFace: {'‚úÖ Ready' if HF_TOKEN else '‚ùå Not configured'}\")\n",
    "print(f\"  Kaggle: {'‚úÖ Ready' if (KAGGLE_API_TOKEN and KAGGLE_USERNAME) else '‚ùå Not configured'}\")\n",
    "print(f\"  OpenRouter: {'‚úÖ Ready' if OPENROUTER_API_KEY else '‚ùå Not configured'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-014",
   "metadata": {},
   "source": [
    "# SECTION 2: Load ODIR-5K Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-015",
   "metadata": {},
   "source": [
    "## Cell 2.1: Download ODIR-5K Dataset from Kaggle\n",
    "\n",
    "**Using kagglehub for automatic download!**\n",
    "\n",
    "### What This Does:\n",
    "- Downloads ODIR-5K dataset (~8 GB) including:\n",
    "  - 10,000 fundus images (paired left/right)\n",
    "  - Excel metadata file with diagnostic keywords\n",
    "- Copies to Google Drive for persistence  \n",
    "- **First run**: ~5-10 min download + ~5 min copy to Drive\n",
    "- **Subsequent runs**: Instant (uses Drive cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "ODIR_DRIVE_DIR = f\"{DRIVE_DATA}/ODIR-5K\"\n",
    "\n",
    "# Check if already downloaded to Drive\n",
    "if os.path.exists(ODIR_DRIVE_DIR):\n",
    "    # Find the Training Images directory (handle different nesting levels)\n",
    "    possible_paths = [\n",
    "        f\"{ODIR_DRIVE_DIR}/ODIR-5K/ODIR-5K/Training Images\",  # Extra nested\n",
    "        f\"{ODIR_DRIVE_DIR}/ODIR-5K/Training Images\",          # Standard\n",
    "        f\"{ODIR_DRIVE_DIR}/Training Images\",                   # Flat\n",
    "    ]\n",
    "    \n",
    "    ODIR_IMAGES_DIR = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            ODIR_IMAGES_DIR = path\n",
    "            break\n",
    "    \n",
    "    if ODIR_IMAGES_DIR:\n",
    "        image_files = list(Path(ODIR_IMAGES_DIR).glob(\"*.jpg\"))\n",
    "        left_images = [f for f in image_files if '_left' in f.name]\n",
    "        right_images = [f for f in image_files if '_right' in f.name]\n",
    "        \n",
    "        print(\"‚úÖ ODIR-5K found in Google Drive!\")\n",
    "        print(f\"   Images directory: {ODIR_IMAGES_DIR}\")\n",
    "        print(f\"   Total training images: {len(image_files)}\")\n",
    "        print(f\"   Left eye: {len(left_images)}\")\n",
    "        print(f\"   Right eye: {len(right_images)}\")\n",
    "        \n",
    "        if len(left_images) == len(right_images):\n",
    "            print(f\"\\n‚úÖ Paired images validated: {len(left_images)} patients\")\n",
    "            print(\"   Skipping download (using cached data from Drive)\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Warning: Unequal number of left and right images - will re-download\")\n",
    "            shutil.rmtree(ODIR_DRIVE_DIR, ignore_errors=True)\n",
    "            ODIR_IMAGES_DIR = None\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è ODIR-5K directory exists but Training Images not found in expected locations\")\n",
    "        print(f\"   Will re-download...\")\n",
    "        shutil.rmtree(ODIR_DRIVE_DIR, ignore_errors=True)\n",
    "\n",
    "if not os.path.exists(ODIR_DRIVE_DIR) or not ODIR_IMAGES_DIR:\n",
    "    print(\"üì• Downloading ODIR-5K from Kaggle...\")\n",
    "    print(\"   This will take ~5-10 minutes (~8 GB dataset)\\n\")\n",
    "    \n",
    "    # Download using kagglehub\n",
    "    dataset_path = kagglehub.dataset_download(\"andrewmvd/ocular-disease-recognition-odir5k\")\n",
    "    \n",
    "    print(f\"‚úÖ Downloaded to: {dataset_path}\")\n",
    "    \n",
    "    # Explore the FULL downloaded structure\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE DOWNLOADED STRUCTURE:\")\n",
    "    print(\"=\"*80)\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        level = root.replace(dataset_path, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        rel_path = os.path.relpath(root, dataset_path)\n",
    "        print(f\"{indent}{rel_path}/\")\n",
    "        sub_indent = ' ' * 2 * (level + 1)\n",
    "        \n",
    "        # Show all files if less than 10, otherwise show summary\n",
    "        if len(files) <= 10:\n",
    "            for file in files:\n",
    "                print(f\"{sub_indent}{file}\")\n",
    "        else:\n",
    "            for file in files[:3]:\n",
    "                print(f\"{sub_indent}{file}\")\n",
    "            print(f\"{sub_indent}... and {len(files) - 3} more files\")\n",
    "    \n",
    "    # Copy EVERYTHING to Google Drive to avoid missing anything\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìÇ Copying ENTIRE dataset to Google Drive...\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   Destination: {ODIR_DRIVE_DIR}\")\n",
    "    \n",
    "    # Remove existing if present\n",
    "    if os.path.exists(ODIR_DRIVE_DIR):\n",
    "        shutil.rmtree(ODIR_DRIVE_DIR)\n",
    "    \n",
    "    # Copy everything\n",
    "    shutil.copytree(dataset_path, ODIR_DRIVE_DIR)\n",
    "    \n",
    "    print(\"‚úÖ Copy complete!\")\n",
    "    \n",
    "    # Show what we got\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COPIED TO DRIVE:\")\n",
    "    print(\"=\"*80)\n",
    "    for root, dirs, files in os.walk(ODIR_DRIVE_DIR):\n",
    "        level = root.replace(ODIR_DRIVE_DIR, '').count(os.sep)\n",
    "        if level < 3:  # Only show top 3 levels\n",
    "            indent = ' ' * 2 * level\n",
    "            rel_path = os.path.relpath(root, ODIR_DRIVE_DIR)\n",
    "            print(f\"{indent}{rel_path}/ ({len(files)} files, {len(dirs)} dirs)\")\n",
    "    \n",
    "    # Detect Training Images directory\n",
    "    possible_paths = [\n",
    "        f\"{ODIR_DRIVE_DIR}/ODIR-5K/ODIR-5K/Training Images\",\n",
    "        f\"{ODIR_DRIVE_DIR}/ODIR-5K/Training Images\",\n",
    "        f\"{ODIR_DRIVE_DIR}/Training Images\",\n",
    "    ]\n",
    "    \n",
    "    ODIR_IMAGES_DIR = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            ODIR_IMAGES_DIR = path\n",
    "            print(f\"\\n‚úÖ Found Training Images at: {ODIR_IMAGES_DIR}\")\n",
    "            break\n",
    "    \n",
    "    if not ODIR_IMAGES_DIR:\n",
    "        print(f\"\\n‚ùå Could not find Training Images in any expected location\")\n",
    "        raise FileNotFoundError(\"Training Images directory not found after download\")\n",
    "\n",
    "# Final validation - load a sample image\n",
    "if ODIR_IMAGES_DIR and os.path.exists(ODIR_IMAGES_DIR):\n",
    "    image_files = list(Path(ODIR_IMAGES_DIR).glob(\"*.jpg\"))\n",
    "    left_images = [f for f in image_files if '_left' in f.name]\n",
    "    right_images = [f for f in image_files if '_right' in f.name]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training images available:\")\n",
    "    print(f\"   Total: {len(image_files)}\")\n",
    "    print(f\"   Left eye: {len(left_images)}\")\n",
    "    print(f\"   Right eye: {len(right_images)}\")\n",
    "    print(f\"   Paired patients: {len(left_images)}\")\n",
    "    \n",
    "    # Load sample image\n",
    "    sample_images = list(Path(ODIR_IMAGES_DIR).glob(\"*_left.jpg\"))\n",
    "    if sample_images:\n",
    "        from PIL import Image\n",
    "        sample_path = sample_images[0]\n",
    "        sample_img = Image.open(sample_path)\n",
    "        print(f\"\\n‚úÖ Sample image loaded: {sample_img.size}\")\n",
    "        print(f\"   Format: {sample_img.format}, Mode: {sample_img.mode}\")\n",
    "        print(f\"\\nüéâ Dataset ready!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No left images found in: {ODIR_IMAGES_DIR}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Training Images directory not found: {ODIR_IMAGES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-017",
   "metadata": {},
   "source": [
    "## Cell 2.2: Load ODIR-5K Metadata\n",
    "\n",
    "Load the Excel metadata file (downloaded in Cell 2.1 along with images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metadata Excel file path - check both possible locations\n",
    "metadata_paths = [\n",
    "    f\"{DRIVE_DATA}/ODIR-5K/ODIR-5K/data.xlsx\",  # Nested structure\n",
    "    f\"{DRIVE_DATA}/ODIR-5K/data.xlsx\",  # Flat structure\n",
    "    f\"{DRIVE_DATA}/ODIR-5K/full_df.csv\",  # Alternative CSV at root\n",
    "]\n",
    "\n",
    "metadata_path = None\n",
    "for path in metadata_paths:\n",
    "    if os.path.exists(path):\n",
    "        metadata_path = path\n",
    "        break\n",
    "\n",
    "print(f\"Loading ODIR-5K metadata...\")\n",
    "\n",
    "if not metadata_path:\n",
    "    print(f\"‚ùå Metadata not found in any expected location:\")\n",
    "    for path in metadata_paths:\n",
    "        print(f\"   {path}\")\n",
    "    print(f\"\\n   Run Cell 2.1 first to download the dataset from Kaggle!\")\n",
    "    raise FileNotFoundError(f\"Metadata file not found\")\n",
    "\n",
    "print(f\"‚úÖ Found metadata at: {metadata_path}\")\n",
    "\n",
    "# Load metadata\n",
    "if metadata_path.endswith('.csv'):\n",
    "    odir_df = pd.read_csv(metadata_path)\n",
    "else:\n",
    "    odir_df = pd.read_excel(metadata_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded ODIR-5K metadata\")\n",
    "print(f\"   Total patients: {len(odir_df)}\")\n",
    "print(f\"   Columns: {list(odir_df.columns)}\")\n",
    "\n",
    "# Apply TEST_MODE sampling if enabled\n",
    "if TEST_MODE and NUM_TEST_PATIENTS:\n",
    "    odir_df = odir_df.head(NUM_TEST_PATIENTS)\n",
    "    print(f\"\\n‚ö†Ô∏è TEST MODE: Using {NUM_TEST_PATIENTS} patients\")\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(odir_df)} patients\")\n",
    "print(f\"\\nSample row:\")\n",
    "display(odir_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-019",
   "metadata": {},
   "source": [
    "## Cell 2.3: Validate Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dataset structure and find image directory\n",
    "print(\"Validating ODIR-5K dataset structure...\\n\")\n",
    "\n",
    "# Check multiple possible image directory locations\n",
    "image_dir_candidates = [\n",
    "    f\"{DRIVE_DATA}/ODIR-5K/ODIR-5K/Training Images\",  # Nested structure\n",
    "    f\"{DRIVE_DATA}/ODIR-5K/Training Images\",  # Flat structure\n",
    "    f\"{DRIVE_DATA}/ODIR-5K/preprocessed_images\",  # Preprocessed folder\n",
    "]\n",
    "\n",
    "ODIR_IMAGES_DIR = None\n",
    "for dir_path in image_dir_candidates:\n",
    "    if os.path.exists(dir_path):\n",
    "        test_images = list(Path(dir_path).glob(\"*.jpg\"))\n",
    "        if test_images:\n",
    "            ODIR_IMAGES_DIR = dir_path\n",
    "            print(f\"‚úÖ Found images at: {ODIR_IMAGES_DIR}\")\n",
    "            break\n",
    "\n",
    "if not ODIR_IMAGES_DIR:\n",
    "    print(\"‚ùå Training images not found in any expected location:\")\n",
    "    for dir_path in image_dir_candidates:\n",
    "        print(f\"   {dir_path}\")\n",
    "    raise FileNotFoundError(\"Training images directory not found\")\n",
    "\n",
    "# Check images exist\n",
    "image_files = list(Path(ODIR_IMAGES_DIR).glob(\"*.jpg\"))\n",
    "left_images = [f for f in image_files if '_left' in f.name]\n",
    "right_images = [f for f in image_files if '_right' in f.name]\n",
    "\n",
    "print(f\"\\nüìä Images statistics:\")\n",
    "print(f\"   Total images: {len(image_files)}\")\n",
    "print(f\"   Left eye: {len(left_images)}\")\n",
    "print(f\"   Right eye: {len(right_images)}\")\n",
    "\n",
    "# Validate required columns in metadata\n",
    "required_columns = ['ID', 'Patient Age', 'Patient Sex', 'Left-Diagnostic Keywords', 'Right-Diagnostic Keywords']\n",
    "missing_columns = [col for col in required_columns if col not in odir_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"\\n‚ùå Missing required columns: {missing_columns}\")\n",
    "    print(f\"   Available columns: {list(odir_df.columns)}\")\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All required metadata columns present\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values in metadata:\")\n",
    "for col in required_columns:\n",
    "    missing = odir_df[col].isna().sum()\n",
    "    print(f\"  {col}: {missing} ({missing/len(odir_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset validation complete!\")\n",
    "print(f\"   Using images from: {ODIR_IMAGES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-021",
   "metadata": {},
   "source": [
    "## Cell 2.4: Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ODIR-5K DATASET STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Age distribution\n",
    "print(f\"\\nAge Statistics:\")\n",
    "print(f\"  Mean: {odir_df['Patient Age'].mean():.1f} years\")\n",
    "print(f\"  Median: {odir_df['Patient Age'].median():.1f} years\")\n",
    "print(f\"  Range: {odir_df['Patient Age'].min():.0f} - {odir_df['Patient Age'].max():.0f} years\")\n",
    "\n",
    "# Sex distribution\n",
    "print(f\"\\nSex Distribution:\")\n",
    "sex_counts = odir_df['Patient Sex'].value_counts()\n",
    "for sex, count in sex_counts.items():\n",
    "    print(f\"  {sex}: {count} ({count/len(odir_df)*100:.1f}%)\")\n",
    "\n",
    "# Keywords distribution (top 10 most common)\n",
    "from collections import Counter\n",
    "\n",
    "all_keywords = []\n",
    "for keywords_str in pd.concat([odir_df['Left-Diagnostic Keywords'], odir_df['Right-Diagnostic Keywords']]).dropna():\n",
    "    all_keywords.extend([k.strip() for k in str(keywords_str).split(',') if k.strip()])\n",
    "\n",
    "keyword_counts = Counter(all_keywords)\n",
    "top_keywords = keyword_counts.most_common(10)\n",
    "\n",
    "print(f\"\\nTop 10 Disease Keywords:\")\n",
    "for keyword, count in top_keywords:\n",
    "    print(f\"  {keyword}: {count} ({count/(len(odir_df)*2)*100:.1f}%)\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Age distribution\n",
    "axes[0].hist(odir_df['Patient Age'].dropna(), bins=20, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Age Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Age (years)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Sex distribution\n",
    "sex_counts.plot(kind='bar', ax=axes[1], color=['lightcoral', 'lightblue'])\n",
    "axes[1].set_title('Sex Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sex')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Top keywords\n",
    "keywords, counts = zip(*top_keywords)\n",
    "axes[2].barh(range(len(keywords)), counts, color='lightgreen')\n",
    "axes[2].set_yticks(range(len(keywords)))\n",
    "axes[2].set_yticklabels(keywords)\n",
    "axes[2].set_title('Top 10 Disease Keywords', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Frequency')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_RESULTS}/odir_dataset_statistics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Statistics plot saved to {DRIVE_RESULTS}/odir_dataset_statistics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-023",
   "metadata": {},
   "source": [
    "# SECTION 3: Generate Clinical Prompts\n",
    "\n",
    "Use DSPy + OpenRouter to generate 3 clinical prompts per patient:\n",
    "1. **Left eye prompt**: Specific to left eye pathology\n",
    "2. **Right eye prompt**: Specific to right eye pathology\n",
    "3. **Patient-level prompt**: Holistic diagnostic impression (both eyes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-024",
   "metadata": {},
   "source": [
    "## Cell 3.1: Define ODIR Prompt Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class OdirPromptSignature(dspy.Signature):\n",
    "    \"\"\"Generate a clinical diagnostic impression for a retinal fundus image with patient metadata.\n",
    "    \n",
    "    Requirements:\n",
    "    - Use varied medical terminology\n",
    "    - Be concise (1-2 sentences)\n",
    "    - Include relevant clinical features when visible\n",
    "    - Incorporate patient demographics (age, sex) when clinically relevant\n",
    "    - Vary phrasing to avoid repetition\n",
    "    - Specify eye laterality (left/right) in the description\n",
    "    \"\"\"\n",
    "    \n",
    "    image = dspy.InputField(desc=\"Color fundus photograph\")\n",
    "    keywords = dspy.InputField(desc=\"Diagnostic keywords from clinical annotations\")\n",
    "    eye_side = dspy.InputField(desc=\"Eye laterality: 'left', 'right', or 'both'\")\n",
    "    age = dspy.InputField(desc=\"Patient age in years\")\n",
    "    sex = dspy.InputField(desc=\"Patient sex (M/F)\")\n",
    "    style_hint = dspy.InputField(desc=\"Writing style guidance for variation\")\n",
    "    \n",
    "    impression = dspy.OutputField(desc=\"Clinical diagnostic impression\")\n",
    "\n",
    "print(\"‚úÖ OdirPromptSignature defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-026",
   "metadata": {},
   "source": [
    "## Cell 3.2: Create ODIR Prompt Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import random\n",
    "from typing import Optional\n",
    "import re\n",
    "\n",
    "class OdirPromptGenerator(dspy.Module):\n",
    "    \"\"\"Generates varied clinical prompts for ODIR-5K fundus images with metadata.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_chain_of_thought: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Choose between simple prediction or chain-of-thought\n",
    "        if use_chain_of_thought:\n",
    "            self.generate = dspy.ChainOfThought(OdirPromptSignature)\n",
    "        else:\n",
    "            self.generate = dspy.Predict(OdirPromptSignature)\n",
    "        \n",
    "        # Style variations for randomization\n",
    "        self.writing_styles = [\n",
    "            \"formal and detailed\",\n",
    "            \"concise and direct\",\n",
    "            \"descriptive with key findings\",\n",
    "            \"focused on diagnostic features\",\n",
    "            \"educational clinical note style\",\n",
    "            \"brief assessment format\",\n",
    "        ]\n",
    "        \n",
    "        self.perspectives = [\n",
    "            \"describe visible pathology\",\n",
    "            \"summarize diagnostic impression\",\n",
    "            \"note clinical significance\",\n",
    "            \"describe characteristic findings\",\n",
    "            \"identify key abnormalities\",\n",
    "        ]\n",
    "        \n",
    "        self.detail_levels = [\"brief\", \"moderate\", \"detailed\"]\n",
    "    \n",
    "    def _create_style_hint(self, rng: random.Random) -> str:\n",
    "        \"\"\"Create a randomized style hint.\"\"\"\n",
    "        style = rng.choice(self.writing_styles)\n",
    "        perspective = rng.choice(self.perspectives)\n",
    "        detail = rng.choice(self.detail_levels)\n",
    "        \n",
    "        return f\"{detail} description, {style}, {perspective}\"\n",
    "    \n",
    "    def forward(self, image, keywords: str, eye_side: str, age: int, sex: str, \n",
    "                rng: Optional[random.Random] = None):\n",
    "        \"\"\"Generate a clinical prompt for the given image and metadata.\"\"\"\n",
    "        rng = rng or random.Random()\n",
    "        \n",
    "        style_hint = self._create_style_hint(rng)\n",
    "        \n",
    "        result = self.generate(\n",
    "            image=image,\n",
    "            keywords=keywords,\n",
    "            eye_side=eye_side,\n",
    "            age=age,\n",
    "            sex=sex,\n",
    "            style_hint=style_hint\n",
    "        )\n",
    "        \n",
    "        # Clean up the output\n",
    "        impression = result.impression.strip()\n",
    "        \n",
    "        # Ensure it ends with a period\n",
    "        if not impression.endswith(('.', '!', '?')):\n",
    "            impression += '.'\n",
    "        \n",
    "        # Take only first sentence if multiple were generated\n",
    "        if '\\n' in impression:\n",
    "            impression = impression.split('\\n')[0].strip()\n",
    "        \n",
    "        # Remove model artifacts\n",
    "        impression = re.sub(r'\\s*\\[\\[\\s*##\\s*completed\\s*##\\s*\\]\\]\\.?', '', impression)\n",
    "        impression = impression.strip()\n",
    "        \n",
    "        # Ensure it still ends with a period after cleanup\n",
    "        if impression and not impression.endswith(('.', '!', '?')):\n",
    "            impression += '.'\n",
    "        \n",
    "        return impression\n",
    "\n",
    "print(\"‚úÖ OdirPromptGenerator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-028",
   "metadata": {},
   "source": [
    "## Cell 3.3: Test Generator on Single Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Initialize generator\n",
    "generator = OdirPromptGenerator(use_chain_of_thought=False)\n",
    "\n",
    "# Get first patient for testing\n",
    "test_patient = odir_df.iloc[0]\n",
    "patient_id = test_patient['ID']\n",
    "age = int(test_patient['Patient Age'])\n",
    "sex = test_patient['Patient Sex']\n",
    "left_keywords = str(test_patient['Left-Diagnostic Keywords'])\n",
    "right_keywords = str(test_patient['Right-Diagnostic Keywords'])\n",
    "\n",
    "# Load images from file paths\n",
    "left_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_left.jpg\"\n",
    "right_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_right.jpg\"\n",
    "\n",
    "if not os.path.exists(left_img_path) or not os.path.exists(right_img_path):\n",
    "    print(f\"‚ùå Images not found for patient {patient_id}\")\n",
    "    print(f\"   Expected: {left_img_path}, {right_img_path}\")\n",
    "    print(f\"\\n‚ö†Ô∏è Make sure you've downloaded and extracted ODIR-5K images!\")\n",
    "else:\n",
    "    left_img = Image.open(left_img_path).convert('RGB')\n",
    "    right_img = Image.open(right_img_path).convert('RGB')\n",
    "    \n",
    "    print(f\"Test Patient: {patient_id}\")\n",
    "    print(f\"  Age: {age}, Sex: {sex}\")\n",
    "    print(f\"  Left Keywords: {left_keywords}\")\n",
    "    print(f\"  Right Keywords: {right_keywords}\")\n",
    "    print(f\"  Left Image: {left_img.size}\")\n",
    "    print(f\"  Right Image: {right_img.size}\")\n",
    "    \n",
    "    # Generate 3 prompts\n",
    "    rng = random.Random(42)\n",
    "    \n",
    "    print(\"\\nGenerating prompts...\\n\")\n",
    "    \n",
    "    # 1. Left eye prompt\n",
    "    left_prompt = generator(\n",
    "        image=left_img,\n",
    "        keywords=left_keywords,\n",
    "        eye_side=\"left\",\n",
    "        age=age,\n",
    "        sex=sex,\n",
    "        rng=rng\n",
    "    )\n",
    "    print(f\"‚úÖ Left Eye Prompt:\\n   {left_prompt}\\n\")\n",
    "    \n",
    "    # 2. Right eye prompt\n",
    "    right_prompt = generator(\n",
    "        image=right_img,\n",
    "        keywords=right_keywords,\n",
    "        eye_side=\"right\",\n",
    "        age=age,\n",
    "        sex=sex,\n",
    "        rng=rng\n",
    "    )\n",
    "    print(f\"‚úÖ Right Eye Prompt:\\n   {right_prompt}\\n\")\n",
    "    \n",
    "    # 3. Patient-level prompt (both eyes)\n",
    "    patient_prompt = generator(\n",
    "        image=left_img,  # Use either image\n",
    "        keywords=f\"{left_keywords}; {right_keywords}\",\n",
    "        eye_side=\"both\",\n",
    "        age=age,\n",
    "        sex=sex,\n",
    "        rng=rng\n",
    "    )\n",
    "    print(f\"‚úÖ Patient-Level Prompt:\\n   {patient_prompt}\\n\")\n",
    "    \n",
    "    print(\"‚úÖ Test generation successful - 3 prompts created per patient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-030",
   "metadata": {},
   "source": [
    "## Cell 3.4: Retry Logic for Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def retry_with_backoff(func, max_retries: int = 5, base_delay: float = 10.0):\n",
    "    \"\"\"Enhanced retry function with exponential backoff and better error handling.\n",
    "    \n",
    "    Handles:\n",
    "    - Rate limiting (429, quota exceeded)\n",
    "    - Bad requests (400, JSON schema errors)\n",
    "    - Model unavailability (404, not found)\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            \n",
    "            # Categorize errors\n",
    "            is_rate_limit = any(kw in error_str for kw in [\"rate limit\", \"429\", \"quota\", \"too many requests\"])\n",
    "            is_bad_request = any(kw in error_str for kw in [\"400\", \"bad request\", \"json schema\"])\n",
    "            is_not_found = any(kw in error_str for kw in [\"404\", \"not found\", \"no endpoints\"])\n",
    "            \n",
    "            # Don't retry 404 errors - model doesn't exist\n",
    "            if is_not_found:\n",
    "                print(f\"‚ùå Model unavailable (404): {error_str[:150]}\")\n",
    "                raise\n",
    "            \n",
    "            # Don't retry JSON schema errors - let DSPy handle fallback\n",
    "            if is_bad_request and \"json schema\" in error_str:\n",
    "                print(f\"‚ùå JSON schema error (will try fallback model)\")\n",
    "                raise\n",
    "            \n",
    "            # Last attempt - give up\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"‚ùå Max retries ({max_retries}) exhausted\")\n",
    "                raise\n",
    "            \n",
    "            # Calculate backoff delay\n",
    "            if is_rate_limit:\n",
    "                delay = base_delay * (2 ** attempt) + random.uniform(0, 5)\n",
    "                print(f\"‚è≥ Rate limited. Waiting {delay:.1f}s (retry {attempt + 1}/{max_retries})\")\n",
    "            else:\n",
    "                delay = base_delay + random.uniform(0, 3)\n",
    "                print(f\"‚ö†Ô∏è Error occurred. Waiting {delay:.1f}s (retry {attempt + 1}/{max_retries})\")\n",
    "                print(f\"   Error: {error_str[:150]}\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "\n",
    "print(\"‚úÖ Retry logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-032",
   "metadata": {},
   "source": [
    "## Cell 3.5: Main Prompt Generation Loop\n",
    "\n",
    "**‚ö†Ô∏è This will take ~30 minutes for 100 patients (TEST_MODE) or ~4-5 hours for 5,000 patients (FULL_MODE)**\n",
    "\n",
    "Generates 3 prompts per patient:\n",
    "- Left eye-specific prompt\n",
    "- Right eye-specific prompt  \n",
    "- Patient-level holistic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Output paths\n",
    "prompts_csv_path = f\"{DRIVE_PROMPTS}/odir_retclip_prompts.csv\"\n",
    "checkpoint_path = f\"{DRIVE_PROMPTS}/generation_checkpoint.json\"\n",
    "\n",
    "# Load checkpoint if exists\n",
    "processed_patients = set()\n",
    "prompts_rows = []\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    with open(checkpoint_path, 'r') as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "        processed_patients = set(checkpoint_data.get('processed_patients', []))\n",
    "        print(f\"‚úÖ Resuming from checkpoint: {len(processed_patients)} patients already processed\")\n",
    "    \n",
    "    # Load existing prompts\n",
    "    if os.path.exists(prompts_csv_path):\n",
    "        existing_df = pd.read_csv(prompts_csv_path)\n",
    "        prompts_rows = existing_df.to_dict('records')\n",
    "\n",
    "print(f\"\\nGenerating prompts for {len(odir_df)} patients...\")\n",
    "print(f\"Checkpoint interval: {CHECKPOINT_INTERVAL}\")\n",
    "print(f\"Delay between calls: {DELAY_BETWEEN_CALLS}s\\n\")\n",
    "\n",
    "# Initialize generator\n",
    "generator = OdirPromptGenerator(use_chain_of_thought=False)\n",
    "\n",
    "# Process each patient\n",
    "for idx, row in tqdm(odir_df.iterrows(), total=len(odir_df), desc=\"Generating prompts\"):\n",
    "    patient_id = row['ID']\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if patient_id in processed_patients:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load patient metadata\n",
    "        age = int(row['Patient Age'])\n",
    "        sex = row['Patient Sex']\n",
    "        left_keywords = str(row['Left-Diagnostic Keywords'])\n",
    "        right_keywords = str(row['Right-Diagnostic Keywords'])\n",
    "        \n",
    "        # Load images from file paths\n",
    "        left_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_left.jpg\"\n",
    "        right_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_right.jpg\"\n",
    "        \n",
    "        if not os.path.exists(left_img_path) or not os.path.exists(right_img_path):\n",
    "            print(f\"\\n‚ö†Ô∏è Images not found for patient {patient_id}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        left_img = Image.open(left_img_path).convert('RGB')\n",
    "        right_img = Image.open(right_img_path).convert('RGB')\n",
    "        \n",
    "        # Create deterministic RNG based on patient ID\n",
    "        rng = random.Random(hash(str(patient_id)) ^ 42)\n",
    "        \n",
    "        # Generate 3 prompts with retry logic\n",
    "        def make_left_prompt():\n",
    "            return generator(\n",
    "                image=left_img,\n",
    "                keywords=left_keywords,\n",
    "                eye_side=\"left\",\n",
    "                age=age,\n",
    "                sex=sex,\n",
    "                rng=rng\n",
    "            )\n",
    "        \n",
    "        def make_right_prompt():\n",
    "            return generator(\n",
    "                image=right_img,\n",
    "                keywords=right_keywords,\n",
    "                eye_side=\"right\",\n",
    "                age=age,\n",
    "                sex=sex,\n",
    "                rng=rng\n",
    "            )\n",
    "        \n",
    "        def make_patient_prompt():\n",
    "            return generator(\n",
    "                image=left_img,\n",
    "                keywords=f\"{left_keywords}; {right_keywords}\",\n",
    "                eye_side=\"both\",\n",
    "                age=age,\n",
    "                sex=sex,\n",
    "                rng=rng\n",
    "            )\n",
    "        \n",
    "        left_prompt = retry_with_backoff(make_left_prompt)\n",
    "        time.sleep(DELAY_BETWEEN_CALLS)\n",
    "        \n",
    "        right_prompt = retry_with_backoff(make_right_prompt)\n",
    "        time.sleep(DELAY_BETWEEN_CALLS)\n",
    "        \n",
    "        patient_prompt = retry_with_backoff(make_patient_prompt)\n",
    "        \n",
    "        # Store results\n",
    "        prompts_rows.append({\n",
    "            'patient_id': patient_id,\n",
    "            'age': age,\n",
    "            'sex': sex,\n",
    "            'left_keywords': left_keywords,\n",
    "            'right_keywords': right_keywords,\n",
    "            'prompt_left': left_prompt,\n",
    "            'prompt_right': right_prompt,\n",
    "            'prompt_patient': patient_prompt\n",
    "        })\n",
    "        \n",
    "        processed_patients.add(patient_id)\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if len(processed_patients) % CHECKPOINT_INTERVAL == 0:\n",
    "            pd.DataFrame(prompts_rows).to_csv(prompts_csv_path, index=False)\n",
    "            with open(checkpoint_path, 'w') as f:\n",
    "                json.dump({'processed_patients': list(processed_patients)}, f)\n",
    "        \n",
    "        # Rate limiting\n",
    "        if idx < len(odir_df) - 1:\n",
    "            time.sleep(DELAY_BETWEEN_CALLS)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing patient {patient_id}: {e}\")\n",
    "        # Save checkpoint on error\n",
    "        pd.DataFrame(prompts_rows).to_csv(prompts_csv_path, index=False)\n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump({'processed_patients': list(processed_patients)}, f)\n",
    "        continue\n",
    "\n",
    "# Save final results\n",
    "prompts_df = pd.DataFrame(prompts_rows)\n",
    "prompts_df.to_csv(prompts_csv_path, index=False)\n",
    "\n",
    "# Clean up checkpoint\n",
    "if os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Prompt generation complete!\")\n",
    "print(f\"   Total patients: {len(prompts_df)}\")\n",
    "print(f\"   Total prompts: {len(prompts_df) * 3} (3 per patient)\")\n",
    "print(f\"   Saved to: {prompts_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-034",
   "metadata": {},
   "source": [
    "# SECTION 4: Preprocess for RET-CLIP\n",
    "\n",
    "Convert generated prompts and images to RET-CLIP format:\n",
    "- **TSV**: Real paired left/right images (patient_id, left_img_base64, right_img_base64)\n",
    "- **JSONL**: Text annotations with eye_side field for tripartite loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-035",
   "metadata": {},
   "source": [
    "## Cell 4.1: Helper Functions for Image Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def image_to_base64_urlsafe(pil_image, size=224):\n",
    "    \"\"\"Convert PIL Image to URL-safe base64 string with resizing\"\"\"\n",
    "    if not isinstance(pil_image, Image.Image):\n",
    "        raise ValueError(f\"Expected PIL Image, got {type(pil_image)}\")\n",
    "    \n",
    "    # Convert to RGB and resize\n",
    "    img = pil_image.convert('RGB')\n",
    "    img = img.resize((size, size), Image.BICUBIC)\n",
    "    \n",
    "    # Encode to base64\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\", quality=95)\n",
    "    img_str = base64.urlsafe_b64encode(buffered.getvalue()).decode()\n",
    "    return img_str\n",
    "\n",
    "print(\"‚úÖ Image encoding helper defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-037",
   "metadata": {},
   "source": [
    "## Cell 4.2: Create TSV File (Paired Images)\n",
    "\n",
    "**Format**: `patient_id\\tleft_img_base64\\tright_img_base64`\n",
    "\n",
    "Real binocular pairs - NOT duplicated monocular images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create TSV file with real paired images\n",
    "tsv_path = f\"{DRIVE_DATA}/odir_train_imgs.tsv\"\n",
    "\n",
    "print(f\"Creating TSV file with paired left/right images...\")\n",
    "print(f\"Output: {tsv_path}\\n\")\n",
    "\n",
    "with open(tsv_path, 'w', encoding='utf-8') as tsv_file:\n",
    "    for idx, row in tqdm(prompts_df.iterrows(), total=len(prompts_df), desc=\"Encoding images\"):\n",
    "        patient_id = row['patient_id']\n",
    "        \n",
    "        # Get image paths\n",
    "        left_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_left.jpg\"\n",
    "        right_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_right.jpg\"\n",
    "        \n",
    "        if not os.path.exists(left_img_path) or not os.path.exists(right_img_path):\n",
    "            print(f\"\\n‚ö†Ô∏è Images not found for patient {patient_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Load and encode both images separately (real binocular pair!)\n",
    "        left_img = Image.open(left_img_path)\n",
    "        right_img = Image.open(right_img_path)\n",
    "        \n",
    "        left_b64 = image_to_base64_urlsafe(left_img, IMAGE_SIZE)\n",
    "        right_b64 = image_to_base64_urlsafe(right_img, IMAGE_SIZE)\n",
    "        \n",
    "        # Write TSV line: patient_id, left_img, right_img\n",
    "        tsv_file.write(f\"{patient_id}\\t{left_b64}\\t{right_b64}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ TSV file created: {tsv_path}\")\n",
    "\n",
    "# Validate format\n",
    "with open(tsv_path, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline().strip()\n",
    "    parts = first_line.split('\\t')\n",
    "    print(f\"\\nValidation:\")\n",
    "    print(f\"  Columns: {len(parts)} (expected: 3)\")\n",
    "    print(f\"  Patient ID: {parts[0]}\")\n",
    "    print(f\"  Left image length: {len(parts[1])} chars\")\n",
    "    print(f\"  Right image length: {len(parts[2])} chars\")\n",
    "    print(f\"  ‚úÖ Format correct - Real binocular pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-039",
   "metadata": {},
   "source": [
    "## Cell 4.3: Create JSONL File (Text Annotations with Eye Side)\n",
    "\n",
    "**Format**: Each patient generates 3 JSONL entries:\n",
    "```json\n",
    "{\"text_id\": 0, \"text\": \"left prompt\", \"image_ids\": [\"patient_id\"], \"eye_side\": \"left\"}\n",
    "{\"text_id\": 1, \"text\": \"right prompt\", \"image_ids\": [\"patient_id\"], \"eye_side\": \"right\"}\n",
    "{\"text_id\": 2, \"text\": \"patient prompt\", \"image_ids\": [\"patient_id\"], \"eye_side\": \"both\"}\n",
    "```\n",
    "\n",
    "The `eye_side` field is used by RET-CLIP's tripartite loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create JSONL file with text annotations and eye_side field\n",
    "jsonl_path = f\"{DRIVE_DATA}/odir_train_texts.jsonl\"\n",
    "\n",
    "print(f\"Creating JSONL file with eye_side annotations...\")\n",
    "print(f\"Output: {jsonl_path}\\n\")\n",
    "\n",
    "text_id = 0\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for idx, row in tqdm(prompts_df.iterrows(), total=len(prompts_df), desc=\"Writing JSONL\"):\n",
    "        patient_id = row['patient_id']\n",
    "        \n",
    "        # Entry 1: Left eye prompt\n",
    "        left_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_left'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"left\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(left_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "        \n",
    "        # Entry 2: Right eye prompt\n",
    "        right_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_right'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"right\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(right_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "        \n",
    "        # Entry 3: Patient-level prompt (both eyes)\n",
    "        patient_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_patient'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"both\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(patient_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "\n",
    "print(f\"\\n‚úÖ JSONL file created: {jsonl_path}\")\n",
    "print(f\"   Total patients: {len(prompts_df)}\")\n",
    "print(f\"   Total text entries: {text_id} (3 per patient)\")\n",
    "\n",
    "# Validate format\n",
    "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "    sample_lines = [json.loads(f.readline()) for _ in range(3)]\n",
    "    \n",
    "print(f\"\\nValidation - First patient's 3 prompts:\")\n",
    "for entry in sample_lines:\n",
    "    print(f\"  [{entry['eye_side']}] {entry['text'][:80]}...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Format correct - 3 prompts per patient with eye_side annotations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-041",
   "metadata": {},
   "source": [
    "## Cell 4.4: Train/Test Split\n",
    "\n",
    "Split the prompts into training and testing sets (80/20 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split prompts DataFrame (80/20 split)\n",
    "train_df, test_df = train_test_split(\n",
    "    prompts_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=None  # Can't stratify if some classes have only 1 sample\n",
    ")\n",
    "\n",
    "print(f\"Train patients: {len(train_df)}\")\n",
    "print(f\"Test patients: {len(test_df)}\")\n",
    "\n",
    "# Save splits\n",
    "train_df.to_csv(f\"{DRIVE_DATA}/train_patients.csv\", index=False)\n",
    "test_df.to_csv(f\"{DRIVE_DATA}/test_patients.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Split saved:\")\n",
    "print(f\"   Train: {DRIVE_DATA}/train_patients.csv\")\n",
    "print(f\"   Test: {DRIVE_DATA}/test_patients.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-043",
   "metadata": {},
   "source": [
    "## Cell 4.5: Create Train TSV and JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train TSV file\n",
    "tsv_path = f\"{DRIVE_DATA}/train_imgs.tsv\"  # FIXED: removed \"odir_\" prefix\n",
    "\n",
    "print(f\"Creating TRAIN TSV file...\")\n",
    "print(f\"Output: {tsv_path}\\n\")\n",
    "\n",
    "with open(tsv_path, 'w', encoding='utf-8') as tsv_file:\n",
    "    for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Encoding train images\"):\n",
    "        patient_id = row['patient_id']\n",
    "        \n",
    "        # Get image paths\n",
    "        left_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_left.jpg\"\n",
    "        right_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_right.jpg\"\n",
    "        \n",
    "        if not os.path.exists(left_img_path) or not os.path.exists(right_img_path):\n",
    "            continue\n",
    "        \n",
    "        # Load and encode both images\n",
    "        left_img = Image.open(left_img_path)\n",
    "        right_img = Image.open(right_img_path)\n",
    "        \n",
    "        left_b64 = image_to_base64_urlsafe(left_img, IMAGE_SIZE)\n",
    "        right_b64 = image_to_base64_urlsafe(right_img, IMAGE_SIZE)\n",
    "        \n",
    "        tsv_file.write(f\"{patient_id}\\t{left_b64}\\t{right_b64}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Train TSV created: {tsv_path}\")\n",
    "\n",
    "# Create Train JSONL file\n",
    "jsonl_path = f\"{DRIVE_DATA}/train_texts.jsonl\"  # FIXED: removed \"odir_\" prefix\n",
    "\n",
    "print(f\"\\nCreating TRAIN JSONL file...\")\n",
    "print(f\"Output: {jsonl_path}\\n\")\n",
    "\n",
    "text_id = 0\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Writing train JSONL\"):\n",
    "        patient_id = row['patient_id']\n",
    "        \n",
    "        # Entry 1: Left eye prompt\n",
    "        left_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_left'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"left\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(left_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "        \n",
    "        # Entry 2: Right eye prompt\n",
    "        right_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_right'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"right\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(right_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "        \n",
    "        # Entry 3: Patient-level prompt\n",
    "        patient_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_patient'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"both\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(patient_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Train JSONL created: {jsonl_path}\")\n",
    "print(f\"   Total text entries: {text_id} (3 per patient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-045",
   "metadata": {},
   "source": [
    "## Cell 4.6: Create Test TSV and JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test TSV file\n",
    "tsv_path = f\"{DRIVE_DATA}/test_imgs.tsv\"  # FIXED: removed \"odir_\" prefix\n",
    "\n",
    "print(f\"Creating TEST TSV file...\")\n",
    "print(f\"Output: {tsv_path}\\n\")\n",
    "\n",
    "with open(tsv_path, 'w', encoding='utf-8') as tsv_file:\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Encoding test images\"):\n",
    "        patient_id = row['patient_id']\n",
    "        \n",
    "        # Get image paths\n",
    "        left_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_left.jpg\"\n",
    "        right_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_right.jpg\"\n",
    "        \n",
    "        if not os.path.exists(left_img_path) or not os.path.exists(right_img_path):\n",
    "            continue\n",
    "        \n",
    "        # Load and encode both images\n",
    "        left_img = Image.open(left_img_path)\n",
    "        right_img = Image.open(right_img_path)\n",
    "        \n",
    "        left_b64 = image_to_base64_urlsafe(left_img, IMAGE_SIZE)\n",
    "        right_b64 = image_to_base64_urlsafe(right_img, IMAGE_SIZE)\n",
    "        \n",
    "        tsv_file.write(f\"{patient_id}\\t{left_b64}\\t{right_b64}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Test TSV created: {tsv_path}\")\n",
    "\n",
    "# Create Test JSONL file\n",
    "jsonl_path = f\"{DRIVE_DATA}/test_texts.jsonl\"  # FIXED: removed \"odir_\" prefix\n",
    "\n",
    "print(f\"\\nCreating TEST JSONL file...\")\n",
    "print(f\"Output: {jsonl_path}\\n\")\n",
    "\n",
    "text_id = 0\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Writing test JSONL\"):\n",
    "        patient_id = row['patient_id']\n",
    "        \n",
    "        # Entry 1: Left eye prompt\n",
    "        left_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_left'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"left\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(left_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "        \n",
    "        # Entry 2: Right eye prompt\n",
    "        right_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_right'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"right\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(right_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "        \n",
    "        # Entry 3: Patient-level prompt\n",
    "        patient_entry = {\n",
    "            \"text_id\": text_id,\n",
    "            \"text\": row['prompt_patient'],\n",
    "            \"image_ids\": [patient_id],\n",
    "            \"eye_side\": \"both\"\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(patient_entry, ensure_ascii=False) + '\\n')\n",
    "        text_id += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Test JSONL created: {jsonl_path}\")\n",
    "print(f\"   Total text entries: {text_id} (3 per patient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-047",
   "metadata": {},
   "source": [
    "# SECTION 5: Build LMDB Database\n",
    "\n",
    "Create LMDB databases for efficient data loading during training.\n",
    "\n",
    "LMDB (Lightning Memory-Mapped Database) is a high-performance embedded database that allows fast random access to image data during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-048",
   "metadata": {},
   "source": [
    "## Cell 5.1: Build Train LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LMDB for train set\n",
    "print(\"=\"*80)\n",
    "print(\"Building LMDB for TRAIN set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python /content/retclip/RET_CLIP/preprocess/build_lmdb_dataset_for_RET-CLIP.py \\\n",
    "    --data_dir {DRIVE_DATA} \\\n",
    "    --splits train \\\n",
    "    --lmdb_dir {DRIVE_LMDB}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-050",
   "metadata": {},
   "source": [
    "## Cell 5.2: Build Test LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LMDB for test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Building LMDB for TEST set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python /content/retclip/RET_CLIP/preprocess/build_lmdb_dataset_for_RET-CLIP.py \\\n",
    "    --data_dir {DRIVE_DATA} \\\n",
    "    --splits test \\\n",
    "    --lmdb_dir {DRIVE_LMDB}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-052",
   "metadata": {},
   "source": [
    "## Cell 5.3: Validate LMDB Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate LMDB by reading a few samples\n",
    "import lmdb\n",
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Validating LMDB databases\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name in ['train', 'test']:\n",
    "    lmdb_path = f\"{DRIVE_LMDB}/{split_name}/imgs\"\n",
    "    \n",
    "    if not os.path.exists(lmdb_path):\n",
    "        print(f\"‚ùå LMDB not found: {lmdb_path}\")\n",
    "        continue\n",
    "    \n",
    "    env = lmdb.open(lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        # Try to read first 3 samples\n",
    "        print(f\"\\n{split_name.upper()} LMDB:\")\n",
    "        \n",
    "        # Get list of patient IDs from the corresponding dataframe\n",
    "        if split_name == 'train':\n",
    "            sample_ids = train_df['patient_id'].head(3).tolist()\n",
    "        else:\n",
    "            sample_ids = test_df['patient_id'].head(3).tolist()\n",
    "        \n",
    "        for patient_id in sample_ids:\n",
    "            # FIXED: Convert patient_id to string before encoding\n",
    "            value = txn.get(str(patient_id).encode('utf-8'))\n",
    "            \n",
    "            if value is None:\n",
    "                print(f\"  ‚ö†Ô∏è No data for {patient_id}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                img_left_b64, img_right_b64 = pickle.loads(value)\n",
    "                print(f\"  ‚úÖ {patient_id}: left={len(img_left_b64)} chars, right={len(img_right_b64)} chars\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error unpacking {patient_id}: {e}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(\"\\n‚úÖ LMDB validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-054",
   "metadata": {},
   "source": [
    "# SECTION 6: Train RET-CLIP\n",
    "\n",
    "Train RET-CLIP using contrastive learning on the ODIR-5K dataset with English PubMedBERT text encoder.\n",
    "\n",
    "**Expected Time**:\n",
    "- TEST MODE (100 patients, 2 epochs): ~30 minutes on T4\n",
    "- FULL MODE (5,000 patients, 10 epochs): ~12-15 hours on A100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-055",
   "metadata": {},
   "source": [
    "## Cell 6.1: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training configuration\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Mode: {'TEST' if TEST_MODE else 'FULL'}\")\n",
    "print(f\"  Vision Model: {VISION_MODEL}\")\n",
    "print(f\"  Text Model: {TEXT_MODEL}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"  Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Train samples: {len(train_df)}\")\n",
    "print(f\"  Test samples: {len(test_df)}\")\n",
    "print(f\"  LMDB dir: {DRIVE_LMDB}/train\")\n",
    "print(f\"  Checkpoint dir: {DRIVE_CHECKPOINTS}\")\n",
    "print(\"\\nüí° Note: Using PubMedBERT (medical domain-specific text encoder)\")\n",
    "print(\"‚úÖ Configuration ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-057",
   "metadata": {},
   "source": [
    "## Cell 6.2: Run Training with torchrun\n",
    "\n",
    "**‚ö†Ô∏è This will take ~30 minutes for TEST_MODE or ~12-15 hours for FULL_MODE**\n",
    "\n",
    "Trains RET-CLIP with:\n",
    "- Distributed data parallel (DDP) even on single GPU for efficiency\n",
    "- Checkpoints saved every epoch to Google Drive\n",
    "- Uses tripartite contrastive loss for binocular architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PYTHONPATH for subprocess\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '/content/retclip'\n",
    "print(f\"‚úÖ Set PYTHONPATH to: {os.environ['PYTHONPATH']}\")\n",
    "\n",
    "# Determine which text encoders to train\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    encoders_to_train = TEXT_ENCODERS\n",
    "    print(f\"\\n‚úÖ TEXT ENCODER COMPARISON MODE: Will train {len(encoders_to_train)} models\")\n",
    "else:\n",
    "    # Train only the default TEXT_MODEL\n",
    "    encoders_to_train = [{\n",
    "        \"name\": \"PubMedBERT\",\n",
    "        \"model_id\": TEXT_MODEL,\n",
    "        \"description\": \"Default text encoder\"\n",
    "    }]\n",
    "    print(f\"\\n‚úÖ SINGLE ENCODER MODE: Training only {TEXT_MODEL}\")\n",
    "\n",
    "# Train each text encoder\n",
    "for encoder_idx, encoder_config in enumerate(encoders_to_train):\n",
    "    encoder_name = encoder_config[\"name\"]\n",
    "    encoder_model_id = encoder_config[\"model_id\"]\n",
    "    encoder_desc = encoder_config[\"description\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Training Model {encoder_idx + 1}/{len(encoders_to_train)}: {encoder_name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Description: {encoder_desc}\")\n",
    "    print(f\"  Model ID: {encoder_model_id}\")\n",
    "    print(f\"  Vision Model: {VISION_MODEL}\")\n",
    "    print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Create encoder-specific name for checkpoints\n",
    "    encoder_short_name = encoder_name.lower().replace('-', '').replace(' ', '')\n",
    "    model_name = f\"retclip_odir_{encoder_short_name}\"\n",
    "    \n",
    "    print(f\"  Checkpoint Name: {model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Run training with distributed launcher\n",
    "    !torchrun --nproc_per_node=1 --master_port=29500 \\\n",
    "        /content/retclip/RET_CLIP/training/main.py \\\n",
    "        --train-data {DRIVE_LMDB}/train \\\n",
    "        --batch-size {BATCH_SIZE} \\\n",
    "        --max-epochs {NUM_EPOCHS} \\\n",
    "        --lr {LEARNING_RATE} \\\n",
    "        --warmup {WARMUP_STEPS} \\\n",
    "        --vision-model {VISION_MODEL} \\\n",
    "        --text-model {encoder_model_id} \\\n",
    "        --logs {DRIVE_CHECKPOINTS} \\\n",
    "        --name {model_name} \\\n",
    "        --save-epoch-frequency 1 \\\n",
    "        --skip-aggregate\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed training for {encoder_name}\")\n",
    "    print(f\"   Checkpoints saved to: {DRIVE_CHECKPOINTS}/{model_name}/checkpoints/\")\n",
    "    \n",
    "    if encoder_idx < len(encoders_to_train) - 1:\n",
    "        print(f\"\\n‚è≥ Moving to next encoder...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ALL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    print(f\"Trained {len(encoders_to_train)} models:\")\n",
    "    for encoder_config in encoders_to_train:\n",
    "        encoder_short_name = encoder_config[\"name\"].lower().replace('-', '').replace(' ', '')\n",
    "        print(f\"  ‚úÖ retclip_odir_{encoder_short_name}\")\n",
    "else:\n",
    "    print(f\"Trained 1 model: retclip_odir_pubmedbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-059",
   "metadata": {},
   "source": [
    "## Cell 6.3: Verify Saved Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved checkpoints for all trained models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saved Checkpoints\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine which models were trained\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    model_names = [f\"retclip_odir_{enc['name'].lower().replace('-', '').replace(' ', '')}\" for enc in TEXT_ENCODERS]\n",
    "else:\n",
    "    model_names = [\"retclip_odir_pubmedbert\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    checkpoint_dir = f\"{DRIVE_CHECKPOINTS}/{model_name}/checkpoints\"\n",
    "    \n",
    "    print(f\"\\nüìÅ {model_name}:\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')])\n",
    "        \n",
    "        if checkpoint_files:\n",
    "            print(f\"   ‚úÖ Found {len(checkpoint_files)} checkpoint(s):\")\n",
    "            for ckpt in checkpoint_files:\n",
    "                ckpt_path = f\"{checkpoint_dir}/{ckpt}\"\n",
    "                size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n",
    "                print(f\"      {ckpt} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "            # Verify final checkpoint exists\n",
    "            final_checkpoint = f\"{checkpoint_dir}/epoch_latest.pt\"\n",
    "            if os.path.exists(final_checkpoint):\n",
    "                print(f\"   ‚úÖ Final checkpoint ready: epoch_latest.pt\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Final checkpoint not found: epoch_latest.pt\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå No .pt files found\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Directory not found: {checkpoint_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Checkpoint Verification Complete\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-061",
   "metadata": {},
   "source": [
    "# SECTION 7: Zero-Shot Evaluation\n",
    "\n",
    "Test RET-CLIP's vision-language alignment by computing similarity between image embeddings and text embeddings for all disease classes.\n",
    "\n",
    "Zero-shot evaluation doesn't require training a classifier - it directly measures how well the model aligns images with clinical text descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-062",
   "metadata": {},
   "source": [
    "## Cell 7.1: Prepare Zero-Shot Prompts\n",
    "\n",
    "Get unique disease keywords from test set and create representative prompts for each disease class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare zero-shot classification prompts from test set disease keywords\n",
    "print(\"Preparing zero-shot disease class prompts...\\n\")\n",
    "\n",
    "# Get unique disease keywords from test set\n",
    "disease_classes = []\n",
    "for keywords_str in pd.concat([test_df['left_keywords'], test_df['right_keywords']]).dropna():\n",
    "    for keyword in str(keywords_str).split(','):\n",
    "        disease = keyword.strip()\n",
    "        if disease and disease not in disease_classes and disease != 'nan':\n",
    "            disease_classes.append(disease)\n",
    "\n",
    "print(f\"Found {len(disease_classes)} unique disease keywords in test set\")\n",
    "print(f\"Disease classes: {disease_classes[:10]}...\" if len(disease_classes) > 10 else f\"Disease classes: {disease_classes}\")\n",
    "\n",
    "# Create zero-shot prompts using patient-level prompts from test set\n",
    "# For each disease, find a representative prompt\n",
    "zero_shot_prompts = {}\n",
    "\n",
    "for disease in disease_classes:\n",
    "    # Find test patients with this disease\n",
    "    matching = test_df[\n",
    "        (test_df['left_keywords'].str.contains(disease, na=False, case=False)) |\n",
    "        (test_df['right_keywords'].str.contains(disease, na=False, case=False))\n",
    "    ]\n",
    "    \n",
    "    if len(matching) > 0:\n",
    "        # Use the patient-level prompt from the first matching patient\n",
    "        zero_shot_prompts[disease] = matching.iloc[0]['prompt_patient']\n",
    "    else:\n",
    "        # Fallback: create a simple clinical description\n",
    "        zero_shot_prompts[disease] = f\"Fundus photograph showing {disease}.\"\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(zero_shot_prompts)} zero-shot prompts\")\n",
    "print(f\"\\nSample prompts:\")\n",
    "for disease, prompt in list(zero_shot_prompts.items())[:3]:\n",
    "    print(f\"  [{disease}]: {prompt[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-064",
   "metadata": {},
   "source": [
    "## Cell 7.2: Zero-Shot Evaluation Loop\n",
    "\n",
    "Evaluate all trained models on the test set:\n",
    "- Load each model checkpoint\n",
    "- Encode text embeddings for disease classes\n",
    "- Extract image features from test set\n",
    "- Compute zero-shot predictions via cosine similarity\n",
    "- Calculate accuracy and F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Determine which models to evaluate\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    models_to_evaluate = TEXT_ENCODERS\n",
    "else:\n",
    "    models_to_evaluate = [{\n",
    "        \"name\": \"PubMedBERT\",\n",
    "        \"model_id\": TEXT_MODEL,\n",
    "        \"hf_model_id\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"description\": \"Default text encoder\"\n",
    "    }]\n",
    "\n",
    "# Store results for all models\n",
    "all_results = {}\n",
    "\n",
    "for encoder_config in models_to_evaluate:\n",
    "    encoder_name = encoder_config[\"name\"]\n",
    "    encoder_model_id = encoder_config[\"model_id\"]  # For RET-CLIP config files (dashes)\n",
    "    encoder_hf_id = encoder_config[\"hf_model_id\"]  # For HuggingFace tokenizer (slashes)\n",
    "    encoder_short_name = encoder_name.lower().replace('-', '').replace(' ', '')\n",
    "    model_name = f\"retclip_odir_{encoder_short_name}\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Evaluating: {encoder_name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Config Model ID: {encoder_model_id}\")\n",
    "    print(f\"  HuggingFace ID: {encoder_hf_id}\")\n",
    "    print(f\"  Checkpoint: {model_name}\")\n",
    "    \n",
    "    # Load model\n",
    "    from RET_CLIP.clip.model import CLIP\n",
    "    \n",
    "    vision_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{VISION_MODEL}.json\"\n",
    "    text_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{encoder_model_id}.json\"\n",
    "    \n",
    "    with open(vision_config_path, 'r') as fv, open(text_config_path, 'r') as ft:\n",
    "        model_cfg = json.load(fv)\n",
    "        for k, v in json.load(ft).items():\n",
    "            model_cfg[k] = v\n",
    "    \n",
    "    model = CLIP(**model_cfg)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = f\"{DRIVE_CHECKPOINTS}/{model_name}/checkpoints/epoch_latest.pt\"\n",
    "    print(f\"  Loading: {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle DDP state dict\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace('module.', '')\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"  ‚úÖ Model loaded\")\n",
    "    \n",
    "    # Load tokenizer for this text encoder using HuggingFace ID\n",
    "    print(f\"  Loading tokenizer from HuggingFace: {encoder_hf_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(encoder_hf_id)\n",
    "    \n",
    "    # Encode text embeddings for disease classes IN THE SAME ORDER as disease_classes\n",
    "    print(f\"\\n  Encoding {len(disease_classes)} disease class prompts...\")\n",
    "    print(f\"  Disease classes order: {disease_classes}\")\n",
    "    text_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # IMPORTANT: Iterate in disease_classes order to match labels!\n",
    "        for disease in disease_classes:\n",
    "            prompt = zero_shot_prompts[disease]\n",
    "            \n",
    "            # Tokenize using the loaded tokenizer\n",
    "            text_input = tokenizer(\n",
    "                [prompt],\n",
    "                max_length=77,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to GPU\n",
    "            input_ids = text_input['input_ids'].cuda()\n",
    "            \n",
    "            # Encode text - handle both tuple and tensor returns\n",
    "            text_output = model.encode_text(input_ids)\n",
    "            if isinstance(text_output, tuple):\n",
    "                text_feat = text_output[0]  # Take first element if tuple\n",
    "            else:\n",
    "                text_feat = text_output\n",
    "            \n",
    "            text_feat = F.normalize(text_feat, dim=-1)\n",
    "            text_features.append(text_feat)\n",
    "    \n",
    "    text_features = torch.cat(text_features, dim=0)\n",
    "    print(f\"  ‚úÖ Text features shape: {text_features.shape}\")\n",
    "    \n",
    "    # Extract image features from test set\n",
    "    print(f\"\\n  Extracting image features from {len(test_df)} test patients...\")\n",
    "    image_features = []\n",
    "    true_labels = []\n",
    "    \n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "    \n",
    "    # Image preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"  Processing\"):\n",
    "            patient_id = row['patient_id']\n",
    "            \n",
    "            # Load images\n",
    "            left_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_left.jpg\"\n",
    "            right_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_right.jpg\"\n",
    "            \n",
    "            if not os.path.exists(left_img_path) or not os.path.exists(right_img_path):\n",
    "                continue\n",
    "            \n",
    "            left_img = Image.open(left_img_path).convert('RGB')\n",
    "            right_img = Image.open(right_img_path).convert('RGB')\n",
    "            \n",
    "            left_tensor = preprocess(left_img).unsqueeze(0).cuda()\n",
    "            right_tensor = preprocess(right_img).unsqueeze(0).cuda()\n",
    "            \n",
    "            # RET-CLIP encode_image takes BOTH images as arguments (binocular architecture)\n",
    "            img_output = model.encode_image(left_tensor, right_tensor)\n",
    "            \n",
    "            # Handle both tuple and tensor returns\n",
    "            if isinstance(img_output, tuple):\n",
    "                img_feat = img_output[0]\n",
    "            else:\n",
    "                img_feat = img_output\n",
    "            \n",
    "            img_feat = F.normalize(img_feat, dim=-1)\n",
    "            image_features.append(img_feat)\n",
    "            \n",
    "            # Get true label (use primary keyword from left or right)\n",
    "            left_kw = str(row['left_keywords']).split(',')[0].strip() if pd.notna(row['left_keywords']) else \"\"\n",
    "            right_kw = str(row['right_keywords']).split(',')[0].strip() if pd.notna(row['right_keywords']) else \"\"\n",
    "            primary_keyword = left_kw if left_kw and left_kw in disease_classes else right_kw\n",
    "            \n",
    "            if primary_keyword in disease_classes:\n",
    "                true_labels.append(disease_classes.index(primary_keyword))\n",
    "            else:\n",
    "                # Use first disease class as fallback\n",
    "                true_labels.append(0)\n",
    "    \n",
    "    image_features = torch.cat(image_features, dim=0)\n",
    "    print(f\"  ‚úÖ Image features shape: {image_features.shape}\")\n",
    "    \n",
    "    # Compute zero-shot predictions\n",
    "    print(f\"\\n  Computing zero-shot predictions...\")\n",
    "    with torch.no_grad():\n",
    "        # Cosine similarity between images and texts\n",
    "        similarity = (image_features @ text_features.T)  # [N_test, N_classes]\n",
    "        predictions = similarity.argmax(dim=-1).cpu().numpy()\n",
    "    \n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n  üìä Results for {encoder_name}:\")\n",
    "    print(f\"     Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"     F1 (Macro): {f1_macro * 100:.2f}%\")\n",
    "    print(f\"     F1 (Weighted): {f1_weighted * 100:.2f}%\")\n",
    "    \n",
    "    # Store results\n",
    "    all_results[encoder_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"predictions\": predictions,\n",
    "        \"true_labels\": true_labels,\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_path = f\"{DRIVE_RESULTS}/zeroshot_metrics_{encoder_short_name}.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"encoder_name\": encoder_name,\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"f1_macro\": float(f1_macro),\n",
    "            \"f1_weighted\": float(f1_weighted),\n",
    "            \"num_test_samples\": len(true_labels),\n",
    "            \"num_classes\": len(disease_classes)\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"  ‚úÖ Metrics saved to: {metrics_path}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ZERO-SHOT EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    print(f\"\\nResults Summary:\")\n",
    "    for encoder_name, results in all_results.items():\n",
    "        print(f\"\\n  {encoder_name}:\")\n",
    "        print(f\"    Accuracy: {results['accuracy'] * 100:.2f}%\")\n",
    "        print(f\"    F1 (Macro): {results['f1_macro'] * 100:.2f}%\")\n",
    "        print(f\"    F1 (Weighted): {results['f1_weighted'] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-066",
   "metadata": {},
   "source": [
    "## Cell 7.3: Visualize Zero-Shot Confusion Matrices\n",
    "\n",
    "Create confusion matrices for all evaluated models to visualize zero-shot classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Check if we have results to visualize\n",
    "if not all_results:\n",
    "    print(\"‚ö†Ô∏è  No results to visualize!\")\n",
    "else:\n",
    "    # Visualize confusion matrices for all models\n",
    "    num_models = len(all_results)\n",
    "    \n",
    "    if num_models == 1:\n",
    "        # Single model - larger single plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "        axes = [ax]\n",
    "    else:\n",
    "        # Multiple models - horizontal layout\n",
    "        fig, axes = plt.subplots(1, num_models, figsize=(10 * num_models, 8))\n",
    "    \n",
    "    for idx, (encoder_name, results) in enumerate(all_results.items()):\n",
    "        cm = confusion_matrix(results['true_labels'], results['predictions'])\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot with smaller annotations and adjusted styling\n",
    "        sns.heatmap(\n",
    "            cm_norm,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='Blues',\n",
    "            xticklabels=disease_classes,\n",
    "            yticklabels=disease_classes,\n",
    "            ax=axes[idx],\n",
    "            cbar_kws={'label': 'Normalized Count'},\n",
    "            annot_kws={'fontsize': 7},  # Smaller annotation font\n",
    "            vmin=0,\n",
    "            vmax=1\n",
    "        )\n",
    "        \n",
    "        # Title\n",
    "        axes[idx].set_title(\n",
    "            f'{encoder_name}\\nZero-Shot Accuracy: {results[\"accuracy\"]*100:.2f}%',\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            pad=15\n",
    "        )\n",
    "        \n",
    "        # Axis labels\n",
    "        axes[idx].set_xlabel('Predicted Disease', fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_ylabel('True Disease', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Tick labels - rotate and adjust size\n",
    "        axes[idx].set_xticklabels(\n",
    "            axes[idx].get_xticklabels(),\n",
    "            rotation=45,\n",
    "            ha='right',\n",
    "            fontsize=8\n",
    "        )\n",
    "        axes[idx].set_yticklabels(\n",
    "            axes[idx].get_yticklabels(),\n",
    "            rotation=0,\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with high DPI for better quality\n",
    "    plt.savefig(\n",
    "        f'{DRIVE_RESULTS}/zeroshot_confusion_matrices.png',\n",
    "        dpi=200,\n",
    "        bbox_inches='tight',\n",
    "        facecolor='white'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Confusion matrices saved to {DRIVE_RESULTS}/zeroshot_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-068",
   "metadata": {},
   "source": [
    "# SECTION 8: Linear Probing Evaluation\n",
    "\n",
    "Test the quality of learned visual representations by training a simple linear classifier (logistic regression) on frozen features.\n",
    "\n",
    "Linear probing freezes the model weights and only trains a classifier on top, measuring how linearly separable the learned features are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-069",
   "metadata": {},
   "source": [
    "## Cell 8.1: Extract Features from Train and Test Sets\n",
    "\n",
    "Extract frozen image features from all trained models to use for linear classifier training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Helper function to extract features from a dataset\n",
    "def extract_features(model, dataframe, desc=\"Extracting features\"):\n",
    "    \"\"\"Extract image features from a dataframe of patients.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=desc):\n",
    "            patient_id = row['patient_id']\n",
    "            \n",
    "            # Load images\n",
    "            left_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_left.jpg\"\n",
    "            right_img_path = f\"{ODIR_IMAGES_DIR}/{patient_id}_right.jpg\"\n",
    "            \n",
    "            if not os.path.exists(left_img_path) or not os.path.exists(right_img_path):\n",
    "                continue\n",
    "            \n",
    "            left_img = Image.open(left_img_path).convert('RGB')\n",
    "            right_img = Image.open(right_img_path).convert('RGB')\n",
    "            \n",
    "            left_tensor = preprocess(left_img).unsqueeze(0).cuda()\n",
    "            right_tensor = preprocess(right_img).unsqueeze(0).cuda()\n",
    "            \n",
    "            # Encode binocular pair (RET-CLIP binocular architecture expects both images)\n",
    "            img_output = model.encode_image(left_tensor, right_tensor)\n",
    "            \n",
    "            # Handle tuple return (some models return tuple of features)\n",
    "            if isinstance(img_output, tuple):\n",
    "                img_feat = img_output[0]\n",
    "            else:\n",
    "                img_feat = img_output\n",
    "            \n",
    "            # Normalize features\n",
    "            img_feat = F.normalize(img_feat, dim=-1)\n",
    "            \n",
    "            features.append(img_feat.cpu())\n",
    "            \n",
    "            # Get true label (use primary keyword)\n",
    "            left_kw = str(row['left_keywords']).split(',')[0].strip() if pd.notna(row['left_keywords']) else \"\"\n",
    "            right_kw = str(row['right_keywords']).split(',')[0].strip() if pd.notna(row['right_keywords']) else \"\"\n",
    "            primary_keyword = left_kw if left_kw and left_kw in disease_classes else right_kw\n",
    "            \n",
    "            if primary_keyword in disease_classes:\n",
    "                labels.append(disease_classes.index(primary_keyword))\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    \n",
    "    features = torch.cat(features, dim=0).numpy()\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Store features for all models\n",
    "all_linear_probe_data = {}\n",
    "\n",
    "# Determine which models to evaluate\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    models_to_evaluate = TEXT_ENCODERS\n",
    "else:\n",
    "    models_to_evaluate = [{\n",
    "        \"name\": \"PubMedBERT\",\n",
    "        \"model_id\": TEXT_MODEL,\n",
    "        \"description\": \"Default text encoder\"\n",
    "    }]\n",
    "\n",
    "for encoder_config in models_to_evaluate:\n",
    "    encoder_name = encoder_config[\"name\"]\n",
    "    encoder_model_id = encoder_config[\"model_id\"]\n",
    "    encoder_short_name = encoder_name.lower().replace('-', '').replace(' ', '')\n",
    "    model_name = f\"retclip_odir_{encoder_short_name}\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Extracting features for: {encoder_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load model\n",
    "    from RET_CLIP.clip.model import CLIP\n",
    "    \n",
    "    vision_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{VISION_MODEL}.json\"\n",
    "    text_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{encoder_model_id}.json\"\n",
    "    \n",
    "    with open(vision_config_path, 'r') as fv, open(text_config_path, 'r') as ft:\n",
    "        model_cfg = json.load(fv)\n",
    "        for k, v in json.load(ft).items():\n",
    "            model_cfg[k] = v\n",
    "    \n",
    "    model = CLIP(**model_cfg)\n",
    "    \n",
    "    checkpoint_path = f\"{DRIVE_CHECKPOINTS}/{model_name}/checkpoints/epoch_latest.pt\"\n",
    "    print(f\"  Loading: {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle DDP state dict\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace('module.', '')\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"  ‚úÖ Model loaded\")\n",
    "    \n",
    "    # Extract train features\n",
    "    print(f\"\\n  Extracting train features from {len(train_df)} patients...\")\n",
    "    train_features, train_labels = extract_features(model, train_df, desc=f\"  Train\")\n",
    "    print(f\"  ‚úÖ Train features: {train_features.shape}\")\n",
    "    \n",
    "    # Extract test features\n",
    "    print(f\"\\n  Extracting test features from {len(test_df)} patients...\")\n",
    "    test_features, test_labels = extract_features(model, test_df, desc=f\"  Test\")\n",
    "    print(f\"  ‚úÖ Test features: {test_features.shape}\")\n",
    "    \n",
    "    # Store data\n",
    "    all_linear_probe_data[encoder_name] = {\n",
    "        \"train_features\": train_features,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"test_features\": test_features,\n",
    "        \"test_labels\": test_labels,\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Feature extraction complete for all models\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-071",
   "metadata": {},
   "source": [
    "## Cell 8.2: Train Logistic Regression Classifiers\n",
    "\n",
    "Train a linear classifier on frozen features for each model and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import json\n",
    "\n",
    "# Store linear probe results\n",
    "all_linear_probe_results = {}\n",
    "\n",
    "for encoder_name, data in all_linear_probe_data.items():\n",
    "    encoder_short_name = encoder_name.lower().replace('-', '').replace(' ', '')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Training linear classifier for: {encoder_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Train logistic regression\n",
    "    print(\"  Training logistic regression...\")\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    clf.fit(data['train_features'], data['train_labels'])\n",
    "    print(\"  ‚úÖ Training complete\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"  Evaluating on test set...\")\n",
    "    test_predictions = clf.predict(data['test_features'])\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(data['test_labels'], test_predictions)\n",
    "    f1_macro = f1_score(data['test_labels'], test_predictions, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(data['test_labels'], test_predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n  üìä Linear Probe Results for {encoder_name}:\")\n",
    "    print(f\"     Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"     F1 (Macro): {f1_macro * 100:.2f}%\")\n",
    "    print(f\"     F1 (Weighted): {f1_weighted * 100:.2f}%\")\n",
    "    \n",
    "    # Store results\n",
    "    all_linear_probe_results[encoder_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"predictions\": test_predictions,\n",
    "        \"true_labels\": data['test_labels'],\n",
    "        \"classifier\": clf\n",
    "    }\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = f\"{DRIVE_RESULTS}/linear_probe_metrics_{encoder_short_name}.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"encoder_name\": encoder_name,\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"f1_macro\": float(f1_macro),\n",
    "            \"f1_weighted\": float(f1_weighted),\n",
    "            \"num_train_samples\": len(data['train_labels']),\n",
    "            \"num_test_samples\": len(data['test_labels']),\n",
    "            \"num_classes\": len(disease_classes)\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"  ‚úÖ Metrics saved to: {metrics_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ LINEAR PROBING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    print(f\"\\nResults Summary:\")\n",
    "    for encoder_name, results in all_linear_probe_results.items():\n",
    "        print(f\"\\n  {encoder_name}:\")\n",
    "        print(f\"    Accuracy: {results['accuracy'] * 100:.2f}%\")\n",
    "        print(f\"    F1 (Macro): {results['f1_macro'] * 100:.2f}%\")\n",
    "        print(f\"    F1 (Weighted): {results['f1_weighted'] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-073",
   "metadata": {},
   "source": [
    "## Cell 8.3: Visualize Linear Probing Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Check if we have results to visualize\n",
    "if not all_linear_probe_results:\n",
    "    print(\"‚ö†Ô∏è  No results to visualize!\")\n",
    "else:\n",
    "    # Create confusion matrices for all models\n",
    "    num_models = len(all_linear_probe_results)\n",
    "    \n",
    "    if num_models == 1:\n",
    "        # Single model - larger single plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "        axes = [ax]\n",
    "    else:\n",
    "        # Multiple models - horizontal layout\n",
    "        fig, axes = plt.subplots(1, num_models, figsize=(10 * num_models, 8))\n",
    "    \n",
    "    for idx, (encoder_name, results) in enumerate(all_linear_probe_results.items()):\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(results['true_labels'], results['predictions'])\n",
    "        \n",
    "        # Normalize by row (true labels)\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot with smaller annotations and adjusted styling\n",
    "        sns.heatmap(\n",
    "            cm_norm,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='Greens',\n",
    "            xticklabels=disease_classes,\n",
    "            yticklabels=disease_classes,\n",
    "            ax=axes[idx],\n",
    "            cbar_kws={'label': 'Normalized Count'},\n",
    "            annot_kws={'fontsize': 7},  # Smaller annotation font\n",
    "            vmin=0,\n",
    "            vmax=1\n",
    "        )\n",
    "        \n",
    "        # Title\n",
    "        axes[idx].set_title(\n",
    "            f'{encoder_name}\\nLinear Probe Accuracy: {results[\"accuracy\"]*100:.2f}%',\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            pad=15\n",
    "        )\n",
    "        \n",
    "        # Axis labels\n",
    "        axes[idx].set_xlabel('Predicted Disease', fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_ylabel('True Disease', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Tick labels - rotate and adjust size\n",
    "        axes[idx].set_xticklabels(\n",
    "            axes[idx].get_xticklabels(),\n",
    "            rotation=45,\n",
    "            ha='right',\n",
    "            fontsize=8\n",
    "        )\n",
    "        axes[idx].set_yticklabels(\n",
    "            axes[idx].get_yticklabels(),\n",
    "            rotation=0,\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with high DPI for better quality\n",
    "    plt.savefig(\n",
    "        f'{DRIVE_RESULTS}/linear_probe_confusion_matrices.png',\n",
    "        dpi=200,\n",
    "        bbox_inches='tight',\n",
    "        facecolor='white'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Confusion matrices saved to {DRIVE_RESULTS}/linear_probe_confusion_matrices.png\")\n",
    "    \n",
    "    # Also save individual confusion matrices for each model\n",
    "    for encoder_name, results in all_linear_probe_results.items():\n",
    "        encoder_short_name = encoder_name.lower().replace('-', '').replace(' ', '')\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(results['true_labels'], results['predictions'])\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Create single plot\n",
    "        fig_single, ax_single = plt.subplots(figsize=(12, 10))\n",
    "        sns.heatmap(\n",
    "            cm_norm,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='Greens',\n",
    "            xticklabels=disease_classes,\n",
    "            yticklabels=disease_classes,\n",
    "            ax=ax_single,\n",
    "            cbar_kws={'label': 'Normalized Count'},\n",
    "            annot_kws={'fontsize': 7},\n",
    "            vmin=0,\n",
    "            vmax=1\n",
    "        )\n",
    "        \n",
    "        ax_single.set_title(\n",
    "            f'Linear Probe: {encoder_name}\\nAccuracy: {results[\"accuracy\"]*100:.2f}%',\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            pad=15\n",
    "        )\n",
    "        ax_single.set_xlabel('Predicted Disease', fontsize=11, fontweight='bold')\n",
    "        ax_single.set_ylabel('True Disease', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        ax_single.set_xticklabels(\n",
    "            ax_single.get_xticklabels(),\n",
    "            rotation=45,\n",
    "            ha='right',\n",
    "            fontsize=8\n",
    "        )\n",
    "        ax_single.set_yticklabels(\n",
    "            ax_single.get_yticklabels(),\n",
    "            rotation=0,\n",
    "            fontsize=8\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        cm_path = f\"{DRIVE_RESULTS}/linear_probe_cm_{encoder_short_name}.png\"\n",
    "        fig_single.savefig(cm_path, dpi=200, bbox_inches='tight', facecolor='white')\n",
    "        plt.close(fig_single)\n",
    "        \n",
    "        print(f\"  ‚úÖ Saved: {cm_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-075",
   "metadata": {},
   "source": [
    "# SECTION 9: Final Report\n",
    "\n",
    "Generate a comprehensive report comparing all trained models and summarizing the entire pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-076",
   "metadata": {},
   "source": [
    "## Cell 9.1: Text Encoder Comparison Table\n",
    "\n",
    "Compare zero-shot and linear probing performance across all text encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for encoder_name in all_results.keys():\n",
    "    zs_results = all_results[encoder_name]\n",
    "    lp_results = all_linear_probe_results[encoder_name]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        \"Text Encoder\": encoder_name,\n",
    "        \"Zero-Shot Accuracy\": f\"{zs_results['accuracy'] * 100:.2f}%\",\n",
    "        \"Zero-Shot F1 (Macro)\": f\"{zs_results['f1_macro'] * 100:.2f}%\",\n",
    "        \"Zero-Shot F1 (Weighted)\": f\"{zs_results['f1_weighted'] * 100:.2f}%\",\n",
    "        \"Linear Probe Accuracy\": f\"{lp_results['accuracy'] * 100:.2f}%\",\n",
    "        \"Linear Probe F1 (Macro)\": f\"{lp_results['f1_macro'] * 100:.2f}%\",\n",
    "        \"Linear Probe F1 (Weighted)\": f\"{lp_results['f1_weighted'] * 100:.2f}%\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*120)\n",
    "print(\"TEXT ENCODER COMPARISON - ZERO-SHOT VS LINEAR PROBING\")\n",
    "print(\"=\"*120)\n",
    "print()\n",
    "display(comparison_df)\n",
    "print()\n",
    "\n",
    "# Find best performers\n",
    "zs_best = max(all_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "lp_best = max(all_linear_probe_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "\n",
    "print(f\"üèÜ Best Zero-Shot Performance: {zs_best[0]} ({zs_best[1]['accuracy'] * 100:.2f}%)\")\n",
    "print(f\"üèÜ Best Linear Probe Performance: {lp_best[0]} ({lp_best[1]['accuracy'] * 100:.2f}%)\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_path = f\"{DRIVE_RESULTS}/text_encoder_comparison.csv\"\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"\\n‚úÖ Comparison table saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-078",
   "metadata": {},
   "source": [
    "## Cell 9.2: Generate Comprehensive Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*120)\n",
    "print(\"VERIFYING ALL ARTIFACTS\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Expected artifacts\n",
    "artifacts = {\n",
    "    \"Prompts\": [\n",
    "        f\"{DRIVE_PROMPTS}/odir_retclip_prompts.csv\"\n",
    "    ],\n",
    "    \"Data Splits\": [\n",
    "        f\"{DRIVE_DATA}/train_patients.csv\",\n",
    "        f\"{DRIVE_DATA}/test_patients.csv\",\n",
    "        f\"{DRIVE_DATA}/train_imgs.tsv\",\n",
    "        f\"{DRIVE_DATA}/test_imgs.tsv\",\n",
    "        f\"{DRIVE_DATA}/train_texts.jsonl\",\n",
    "        f\"{DRIVE_DATA}/test_texts.jsonl\"\n",
    "    ],\n",
    "    \"LMDB Databases\": [\n",
    "        f\"{DRIVE_LMDB}/train/imgs\",\n",
    "        f\"{DRIVE_LMDB}/test/imgs\"\n",
    "    ],\n",
    "    \"Model Checkpoints\": [],\n",
    "    \"Results & Metrics\": [\n",
    "        f\"{DRIVE_RESULTS}/odir_dataset_statistics.png\",\n",
    "        f\"{DRIVE_RESULTS}/text_encoder_comparison.csv\",\n",
    "        f\"{DRIVE_RESULTS}/final_report.txt\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add checkpoints for all trained models\n",
    "for encoder_config in (TEXT_ENCODERS if RUN_TEXT_ENCODER_COMPARISON else [{\"name\": \"PubMedBERT\"}]):\n",
    "    encoder_short_name = encoder_config['name'].lower().replace('-', '').replace(' ', '')\n",
    "    artifacts[\"Model Checkpoints\"].append(\n",
    "        f\"{DRIVE_CHECKPOINTS}/retclip_odir_{encoder_short_name}/checkpoints/epoch_latest.pt\"\n",
    "    )\n",
    "    artifacts[\"Results & Metrics\"].extend([\n",
    "        f\"{DRIVE_RESULTS}/zeroshot_metrics_{encoder_short_name}.json\",\n",
    "        f\"{DRIVE_RESULTS}/zeroshot_confusion_matrix_{encoder_short_name}.png\",\n",
    "        f\"{DRIVE_RESULTS}/linear_probe_metrics_{encoder_short_name}.json\",\n",
    "        f\"{DRIVE_RESULTS}/linear_probe_confusion_matrix_{encoder_short_name}.png\"\n",
    "    ])\n",
    "\n",
    "# Add comparison plots if multiple models\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    artifacts[\"Results & Metrics\"].extend([\n",
    "        f\"{DRIVE_RESULTS}/zeroshot_comparison_all_models.png\",\n",
    "        f\"{DRIVE_RESULTS}/linear_probe_comparison_all_models.png\"\n",
    "    ])\n",
    "\n",
    "# Check each artifact\n",
    "total_artifacts = 0\n",
    "found_artifacts = 0\n",
    "missing_artifacts = []\n",
    "\n",
    "for category, paths in artifacts.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for path in paths:\n",
    "        total_artifacts += 1\n",
    "        if os.path.exists(path):\n",
    "            # Get file size\n",
    "            if os.path.isfile(path):\n",
    "                size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "                print(f\"  ‚úÖ {os.path.basename(path)} ({size_mb:.2f} MB)\")\n",
    "                found_artifacts += 1\n",
    "            else:\n",
    "                # Directory (LMDB)\n",
    "                print(f\"  ‚úÖ {os.path.basename(path)} (directory)\")\n",
    "                found_artifacts += 1\n",
    "        else:\n",
    "            print(f\"  ‚ùå {os.path.basename(path)} - NOT FOUND\")\n",
    "            missing_artifacts.append(path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"ARTIFACT SUMMARY: {found_artifacts}/{total_artifacts} found\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "if missing_artifacts:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing {len(missing_artifacts)} artifact(s):\")\n",
    "    for path in missing_artifacts:\n",
    "        print(f\"  - {path}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All artifacts successfully created and saved to Google Drive!\")\n",
    "    print(f\"\\nüìÅ Base directory: {DRIVE_BASE}\")\n",
    "    print(f\"\\nüéâ Pipeline complete! Ready for analysis and publication.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*120)\n",
    "if TEST_MODE:\n",
    "    print(\"\"\"\n",
    "1. ‚úÖ Review test results to ensure pipeline works correctly\n",
    "2. ‚ö†Ô∏è Set TEST_MODE = False in Cell 1.5 for full training\n",
    "3. üöÄ Run full pipeline on all 5,000 patients (~18-24 hours)\n",
    "4. üìä Analyze final results and write research paper\n",
    "5. üìÑ Target venues: MICCAI, IEEE TMI, or similar\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "1. ‚úÖ Full pipeline complete on all patients\n",
    "2. üìä Analyze results and create visualizations\n",
    "3. üìù Write research paper draft\n",
    "4. üî¨ Consider additional experiments:\n",
    "   - Fine-tuning on downstream tasks\n",
    "   - Ablation studies\n",
    "   - Cross-dataset validation\n",
    "5. üìÑ Submit to target venue (MICCAI, IEEE TMI, etc.)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-080",
   "metadata": {},
   "source": [
    "## Cell 9.3: List All Artifacts\n",
    "\n",
    "Verify all output files were successfully created and saved to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x6fkuvtmrig",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all expected artifacts and verify they exist\n",
    "import os\n",
    "\n",
    "print(\"Verifying Output Artifacts:\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Define expected artifacts by category\n",
    "artifacts = {\n",
    "    \"Prompts\": [\n",
    "        f\"{DRIVE_PROMPTS}/odir_retclip_prompts.csv\"\n",
    "    ],\n",
    "    \"Data Splits\": [\n",
    "        f\"{DRIVE_DATA}/train_patients.csv\",\n",
    "        f\"{DRIVE_DATA}/test_patients.csv\",\n",
    "        f\"{DRIVE_DATA}/odir_train_imgs.tsv\",\n",
    "        f\"{DRIVE_DATA}/odir_test_imgs.tsv\",\n",
    "        f\"{DRIVE_DATA}/odir_train_texts.jsonl\",\n",
    "        f\"{DRIVE_DATA}/odir_test_texts.jsonl\"\n",
    "    ],\n",
    "    \"LMDB Databases\": [\n",
    "        f\"{DRIVE_LMDB}/train\",\n",
    "        f\"{DRIVE_LMDB}/test\"\n",
    "    ],\n",
    "    \"Model Checkpoints\": [],\n",
    "    \"Results & Metrics\": [\n",
    "        f\"{DRIVE_RESULTS}/odir_dataset_statistics.png\",\n",
    "        f\"{DRIVE_RESULTS}/zeroshot_confusion_matrices.png\",\n",
    "        f\"{DRIVE_RESULTS}/linear_probe_confusion_matrices.png\",\n",
    "        f\"{DRIVE_RESULTS}/final_report.txt\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add checkpoints for all trained models\n",
    "if RUN_TEXT_ENCODER_COMPARISON:\n",
    "    encoders_list = TEXT_ENCODERS\n",
    "else:\n",
    "    encoders_list = [{\"name\": \"PubMedBERT\", \"model_id\": TEXT_MODEL}]\n",
    "\n",
    "for encoder_config in encoders_list:\n",
    "    encoder_short_name = encoder_config['name'].lower().replace('-', '').replace(' ', '')\n",
    "    model_name = f\"retclip_odir_{encoder_short_name}\"\n",
    "    artifacts[\"Model Checkpoints\"].append(\n",
    "        f\"{DRIVE_CHECKPOINTS}/{model_name}/checkpoints/epoch_latest.pt\"\n",
    "    )\n",
    "\n",
    "# Verify each artifact\n",
    "total_artifacts = 0\n",
    "found_artifacts = 0\n",
    "\n",
    "for category, paths in artifacts.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for path in paths:\n",
    "        total_artifacts += 1\n",
    "        exists = os.path.exists(path)\n",
    "        found_artifacts += exists\n",
    "        \n",
    "        status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "        print(f\"  {status} {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(f\"Artifacts Summary: {found_artifacts}/{total_artifacts} found\")\n",
    "\n",
    "if found_artifacts == total_artifacts:\n",
    "    print(\"‚úÖ All artifacts successfully created!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  {total_artifacts - found_artifacts} artifacts missing - check for errors above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
