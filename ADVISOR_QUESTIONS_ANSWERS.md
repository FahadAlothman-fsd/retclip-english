# Advisor Questions & Answers

## Q1: Are the pretrained weights frozen during training?

### Answer: **NO - Both vision and text encoders are being trained (fine-tuned)**

### Evidence from code:

**File:** [training/main.py:126-134](retclip/RET_CLIP/training/main.py#L126-L134)

```python
if args.freeze_vision:
    for k, v in model.visual.named_parameters():
        v.requires_grad = False
    # freeze bn running mean and variance
    if args.vision_model in ['RN50']:
        for m in model.visual.modules():
            if isinstance(m, torch.nn.BatchNorm2d):
                m.eval()
    logging.info("The visual encoder is freezed during training.")
```

**File:** [training/main.py:396-397](retclip/RET_CLIP/training/main.py#L396-L397)

```python
# At the start of each epoch
for param in model.parameters():
    param.requires_grad = True  # All parameters are trainable
```

### Current Training Configuration:

```python
# Your training command does NOT include --freeze-vision
torchrun --nproc_per_node=1 --master_port=29500 \
    /content/retclip/RET_CLIP/training/main.py \
    --clip-weight-path {CLIP_WEIGHT_PATH} \
    --bert-weight-path {BERT_WEIGHT_PATH} \
    # NO --freeze-vision flag
```

### What this means:

1. **Vision Encoder (ViT-B/16)**:
   - Loads OpenAI CLIP pretrained weights
   - **Weights are NOT frozen** â†’ Gradients are computed and updated
   - Fine-tunes on ODIR-5K retinal images

2. **Text Encoder (PubMedBERT/BERT/BioBERT)**:
   - Loads HuggingFace pretrained weights
   - **Weights are NOT frozen** â†’ Gradients are computed and updated
   - Fine-tunes on ODIR disease descriptions

3. **Training behavior**:
   - Line 397 explicitly sets `param.requires_grad = True` for ALL parameters at each epoch
   - Both encoders adapt to the retinal imaging domain during training
   - Learning rate = 3Ã—10â»âµ (very small) â†’ gentle fine-tuning, not aggressive retraining

---

## Q2: What text encoder architecture are you using?

### Answer: **BERT-based text encoders (NOT CLIP text encoder)**

### Current Text Encoders (all BERT-based):

| Model | Architecture | Training Corpus | Domain |
|-------|--------------|----------------|--------|
| **PubMedBERT** | BERT-base (12 layers) | PubMed abstracts + PMC full-text | Medical |
| **BERT-base** | BERT-base (12 layers) | Wikipedia + BooksCorpus | General |
| **BioBERT** | BERT-base (12 layers) | PubMed + PMC articles | Biomedical |

### Why BERT instead of CLIP text encoder?

**RET-CLIP's design choice:**
- Original RET-CLIP paper uses **Chinese RoBERTa** (BERT-family)
- BERT models have better medical domain pretraining available
- CLIP's text encoder is a Transformer trained on general image captions (not medical text)

### Comparison: BERT vs CLIP Text Encoder

| Feature | BERT (Current) | CLIP Text Encoder |
|---------|----------------|-------------------|
| Architecture | 12-layer Transformer | 12-layer Transformer |
| Input | Text only | Text only |
| Pretraining | Masked Language Modeling (MLM) | Contrastive vision-language |
| Medical versions | âœ… PubMedBERT, BioBERT, ClinicalBERT | âŒ Limited (MedCLIP exists) |
| Domain adaptation | Strong (trained on PubMed) | Weak (trained on image captions) |

---

## Q3: Should you use a CLIP text encoder instead (like MedCLIP)?

### Answer: **Yes, this would be scientifically interesting to compare!**

### Available Medical CLIP Models (2024 Update):

#### 1. **EyeCLIP** (ðŸŽ¯ BEST FOR YOUR PROJECT - Ophthalmology-Specific!)
- **Paper**: "EyeCLIP: A vision-language foundation model for multi-modal ophthalmic image analysis" (2024)
- **GitHub**: `Michi-3000/EyeCLIP`
- **Training**: 2.77 million ophthalmic images across 11 modalities + 11,000 clinical reports
- **Modalities**: OCT, fundus photography, angiography, slit-lamp, etc.
- **Domain**: **Ophthalmology** (specifically designed for retinal imaging!)
- **Architecture**: Dual-encoder CLIP architecture (vision + text encoder)
- **Pros**:
  - âœ… Specifically trained on **retinal fundus images**
  - âœ… Uses clinical ophthalmology reports (similar to RET-CLIP's approach)
  - âœ… 2024 state-of-the-art for ophthalmic AI
  - âœ… Proven on diabetic retinopathy, glaucoma, AMD (your exact use case!)
- **Cons**:
  - May not be on HuggingFace yet (need to check GitHub)
  - Integration effort required

#### 2. **MedCLIP** (General Medical)
- **Paper**: "MedCLIP: Contrastive Learning from Unpaired Medical Images and Text" (2022)
- **Updates**: MedCLIP-SAM (2024) for segmentation tasks
- **Training**: Decoupled learning from unpaired medical images and text
- **Domain**: General medical (X-rays, pathology, radiology)
- **Innovation**: **Semantic matching loss** - doesn't require paired image-text data
- **Pros**:
  - âœ… Zero-shot classification proven effective
  - âœ… Requires only 1/10th the training data of competitors
  - âœ… Outperforms on zero-shot prediction tasks
- **Cons**:
  - âŒ Not ophthalmology-specific (trained on X-rays, pathology)
  - âŒ GitHub-only, not HuggingFace

#### 3. **BiomedCLIP** (Biomedical)
- **Paper**: "BiomedCLIP: A Multimodal Biomedical Foundation Model" (2023)
- **HuggingFace**: `microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224`
- **Training**: 15 million biomedical image-text pairs (PubMed)
- **Domain**: Biomedical (pathology, radiology, histology)
- **Pros**:
  - âœ… Available on HuggingFace
  - âœ… Large-scale biomedical pretraining
  - âœ… Modern CLIP architecture
- **Cons**:
  - âŒ Not retinal-specific
  - âŒ May need architecture changes to integrate
  - âŒ Actually uses **PubMedBERT** as text encoder (same as you!)

#### 4. **OpenAI CLIP ViT-B/16 Text Encoder** (General Baseline)
- **HuggingFace**: `openai/clip-vit-base-patch16`
- **Training**: 400M image-text pairs (general internet)
- **Domain**: General (no medical knowledge)
- **Pros**:
  - âœ… Easy to integrate (already using vision encoder)
  - âœ… Well-studied baseline
- **Cons**:
  - âŒ No medical domain knowledge
  - âŒ Likely worst performance

---

## Proposed Experiment: BERT vs CLIP Text Encoder Comparison

### Setup

Compare **6 text encoders** in total:

**BERT-based (current):**
1. PubMedBERT (medical)
2. BERT-base (general)
3. BioBERT (biomedical)

**CLIP-based (new):**
4. BiomedCLIP text encoder (medical CLIP)
5. OpenAI CLIP text encoder (general CLIP)
6. MedCLIP text encoder (if integration feasible)

### Research Questions

1. **Does CLIP text encoder architecture improve zero-shot performance vs BERT?**
   - Hypothesis: CLIP trained with contrastive learning may align better with vision encoder

2. **Does medical domain pretraining matter more than architecture?**
   - Compare: PubMedBERT (BERT + medical) vs OpenAI CLIP (CLIP + general)
   - Compare: BiomedCLIP (CLIP + medical) vs OpenAI CLIP (CLIP + general)

3. **What's the best combination for retinal imaging?**
   - Current best: PubMedBERT (BERT + medical) = 12.19%
   - Can BiomedCLIP (CLIP + medical) beat it?

### Expected Results

**Prediction:**
1. **BiomedCLIP** > PubMedBERT (better architecture + medical domain)
2. **PubMedBERT** > OpenAI CLIP (domain knowledge matters)
3. **BioBERT/BiomedCLIP** â‰ˆ similar (both biomedical, different architectures)
4. **BERT-base** â‰ˆ **OpenAI CLIP** (both general, different architectures)

---

## Implementation Plan: Adding BiomedCLIP

### Step 1: Check BiomedCLIP Architecture Compatibility

BiomedCLIP uses:
- **Vision Encoder**: ViT-B/16 (same as yours âœ…)
- **Text Encoder**: PubMedBERT with projection layer

**Issue**: BiomedCLIP's text encoder is actually **PubMedBERT**, not a transformer-based CLIP text encoder!

This means:
- You're already effectively using BiomedCLIP's text encoder (PubMedBERT)
- Need to check if BiomedCLIP has different architecture modifications

### Step 2: Try OpenAI CLIP Text Encoder First (Easiest)

Since BiomedCLIP uses BERT anyway, the cleanest comparison is:

**Current:**
- Vision: OpenAI CLIP ViT-B/16
- Text: PubMedBERT (BERT architecture)

**New experiment:**
- Vision: OpenAI CLIP ViT-B/16
- Text: OpenAI CLIP Text Transformer (CLIP architecture)

This tests: **BERT vs CLIP architecture** directly.

### Step 3: Add CLIP Text Encoder to RET-CLIP

You'll need to modify RET-CLIP to support CLIP's text encoder instead of BERT.

**Challenges:**
1. RET-CLIP expects BERT-style tokenization (`WordPiece`)
2. CLIP uses `BPE` tokenization (byte-pair encoding)
3. Different vocabulary sizes and embedding dimensions

**Solution**: Create a new model config for CLIP text encoder

---

## ðŸŽ¯ CRITICAL FINDING: EyeCLIP

### Why EyeCLIP is Perfect for Your Project

**EyeCLIP (2024)** is specifically designed for ophthalmology and directly competes with RET-CLIP:

| Feature | RET-CLIP (Your Current) | EyeCLIP (2024 SOTA) |
|---------|------------------------|---------------------|
| **Training Data** | 193,865 patients (Chinese) | 2.77M images (multi-modal) |
| **Modalities** | Fundus only | 11 modalities (fundus, OCT, angiography) |
| **Text Encoder** | BERT (RoBERTa/PubMedBERT) | CLIP text encoder |
| **Vision Encoder** | ViT-B/16 (OpenAI CLIP) | Custom ophthalmic encoder |
| **Binocular** | âœ… Yes (left/right separate) | Unknown |
| **Diseases** | Multi-label classification | DR, glaucoma, AMD, etc. |
| **Domain** | Retinal (Chinese reports) | Ophthalmic (English reports) |

**Your advisor's question is essentially asking**: **"Should you use EyeCLIP instead of RET-CLIP?"**

---

## Recommendation for Your Advisor

### â­ Option A: Compare RET-CLIP vs EyeCLIP (BEST FOR THESIS)

**Research Question**: Does ophthalmology-specific pretraining (EyeCLIP) outperform general retinal pretraining (RET-CLIP)?

**Approach**:
1. Fine-tune EyeCLIP on ODIR-5K (same as RET-CLIP)
2. Compare zero-shot classification performance
3. Analyze which model better captures retinal disease semantics

**Expected Results**:
- EyeCLIP likely wins (larger dataset, ophthalmology-specific)
- Shows value of domain-specific foundation models
- Publishable comparison (2024 SOTA vs established baseline)

**Timeline**: 2 weeks
- Week 1: Integrate EyeCLIP, fine-tune on ODIR-5K
- Week 2: Evaluate, compare, analyze results

**Thesis Value**: â­â­â­â­â­
- Compares two ophthalmic foundation models
- Tests generalization across datasets (EyeCLIPâ†’ODIR-5K)
- Addresses: "Does scale and modality diversity improve performance?"

---

### Option B: Text Encoder Ablation (BERT vs CLIP)

**Research Question**: Does CLIP text encoder architecture improve zero-shot performance vs BERT?

**Approach**:
1. Replace RET-CLIP's BERT encoder with OpenAI CLIP text encoder
2. Keep vision encoder fixed (ViT-B/16)
3. Compare: BERT (current) vs CLIP text encoder

**Expected Results**:
- BERT wins (medical pretraining > architecture)
- Shows domain knowledge matters more than architecture

**Timeline**: 1 week

**Thesis Value**: â­â­â­
- Answers advisor's architecture question
- Ablation study (isolates text encoder effect)
- Less novel (architecture comparison, not model comparison)

---

### Option C: Comprehensive Benchmark (ALL Models)

**Research Question**: What combination of vision/text encoders works best for retinal disease classification?

**Test 4 complete models**:
1. **RET-CLIP** (ViT-B/16 + PubMedBERT) - Current
2. **EyeCLIP** (Ophthalmic Vision + CLIP Text) - 2024 SOTA
3. **BiomedCLIP** (ViT-B/16 + PubMedBERT) - Biomedical baseline
4. **OpenAI CLIP** (ViT-B/16 + CLIP Text) - General baseline

**Timeline**: 3-4 weeks

**Thesis Value**: â­â­â­â­â­â­ (publication-worthy)
- Comprehensive benchmark
- Shows domain adaptation effects
- First comparison of ophthalmology foundation models on ODIR-5K

---

### Option D: Just Use EyeCLIP (Fastest)

**Simply switch** from RET-CLIP to EyeCLIP:
- Fine-tune EyeCLIP on ODIR-5K
- Report zero-shot performance
- Compare with RET-CLIP baseline (12.19%)

**Timeline**: 1 week

**Thesis Value**: â­â­
- Quick baseline
- No novelty (just applying existing model)
- Good if time-constrained

---

## My Recommendation: **Option A (RET-CLIP vs EyeCLIP)**

### Why Option A is Best:

1. **Directly answers advisor's question**:
   - Uses CLIP text encoder (EyeCLIP) vs BERT (RET-CLIP)
   - Compares architectures on same task

2. **Scientifically interesting**:
   - EyeCLIP (2024) vs RET-CLIP (2024) - both state-of-the-art
   - Tests generalization: EyeCLIP trained on diverse ophthalmology â†’ ODIR-5K retinal
   - Addresses: "Do we need retinal-specific vs general ophthalmology models?"

3. **Thesis contribution**:
   - First benchmark of ophthalmology foundation models on ODIR-5K
   - Shows which pretraining strategy generalizes better
   - Provides guidance for future retinal AI research

4. **Practical impact**:
   - If EyeCLIP wins â†’ Use it for clinical deployment
   - If RET-CLIP wins â†’ Retinal-specific pretraining is crucial

5. **Feasible timeline**: 2 weeks (fits thesis schedule)

---

## What Should You Tell Your Advisor?

### Summary for Advisor Meeting:

**Question 1: Are weights frozen?**

> "No, both the vision encoder (OpenAI CLIP ViT-B/16) and text encoder (PubMedBERT) are fine-tuned on the ODIR-5K dataset. The pretrained weights are loaded as initialization, then updated during training with a learning rate of 3Ã—10â»âµ. I verified this by checking the training code - there's no `--freeze-vision` flag used, and all parameters have `requires_grad=True` during training."

**Question 2: What text encoder are you using?**

> "I'm using BERT-based text encoders (PubMedBERT, BERT-base, BioBERT), not CLIP's text encoder. RET-CLIP was originally designed with BERT-family encoders because they have better medical domain pretraining available. However, I found that there's a newer model called **EyeCLIP (2024)** that uses CLIP text encoders specifically for ophthalmology."

**Question 3: Should you use CLIP text encoder / different model?**

> "Yes! I found **EyeCLIP (2024)**, which is specifically designed for ophthalmology and uses CLIP text encoders. It's the state-of-the-art for ophthalmic imaging.
>
> **Key finding**: EyeCLIP was trained on 2.77 million ophthalmic images (including fundus photography, OCT, angiography) with clinical reports - much larger and more diverse than RET-CLIP's 193K retinal-only images.
>
> **My recommendation**: Compare RET-CLIP vs EyeCLIP on ODIR-5K
>
> **Why this is valuable for the thesis:**
> - Answers your CLIP text encoder question directly (EyeCLIP uses CLIP, RET-CLIP uses BERT)
> - Compares retinal-specific (RET-CLIP) vs general ophthalmology (EyeCLIP) pretraining
> - Tests if larger, multi-modal pretraining beats domain-specific pretraining
> - First benchmark of ophthalmology foundation models on ODIR-5K
> - Only 2 weeks of work (1 week integration + 1 week evaluation)
>
> **Alternative options if you prefer**:
> 1. Just switch to EyeCLIP entirely (1 week, simplest)
> 2. Compare 4 models: RET-CLIP, EyeCLIP, BiomedCLIP, OpenAI CLIP (3-4 weeks, publication-worthy)
> 3. Only test CLIP text encoder within RET-CLIP architecture (1 week, ablation study)
>
> Which direction do you think would be most valuable for the thesis?"

---

## Next Steps

1. **Discuss with advisor** which option to pursue
2. **If Option A/B**: I'll help you integrate CLIP text encoders into RET-CLIP
3. **If Option C**: I'll help you fine-tune BiomedCLIP on ODIR-5K

Let me know what your advisor decides!
