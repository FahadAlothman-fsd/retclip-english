{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RET-CLIP Complete Training & Evaluation Pipeline\n",
    "\n",
    "This notebook implements a comprehensive pipeline for training and evaluating **RET-CLIP** (Retinal-Contrastive Language-Image Pre-training), a foundation model for retinal fundus image analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Background\n",
    "\n",
    "**CLIP (Contrastive Language-Image Pre-training)** [Radford et al., 2021] revolutionized vision-language learning by jointly training image and text encoders using contrastive learning. RET-CLIP adapts this approach for medical imaging, specifically retinal fundus photography.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Contrastive Learning**: Train the model to align images with their corresponding text descriptions while pushing apart mismatched pairs\n",
    "2. **Dual Encoders**: \n",
    "   - **Vision Encoder**: ViT-B-16 (Vision Transformer) for fundus images\n",
    "   - **Text Encoder**: PubMedBERT for clinical descriptions\n",
    "3. **Zero-Shot Classification**: Use text prompts at inference time without task-specific fine-tuning\n",
    "4. **Linear Probing**: Evaluate learned representations by training a simple classifier on frozen features\n",
    "\n",
    "### References:\n",
    "- Radford et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision.\" ICML.\n",
    "- Dosovitskiy et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" ICLR.\n",
    "- Gu et al. (2021). \"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.\" ACM TIST.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Pipeline Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1. Setup & Environment             â”‚  â† Dependencies, GPU, Drive\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  2. Load Pre-Generated Prompts      â”‚  â† CSV with clinical descriptions\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  3. Convert CSV â†’ TSV + JSONL       â”‚  â† Download & encode images\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  4. Build LMDB Database             â”‚  â† Efficient storage format\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  5. Train RET-CLIP (10 epochs)      â”‚  â† Contrastive learning\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  6. Zero-Shot Evaluation            â”‚  â† Vision-language alignment\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  7. Linear Probing Evaluation       â”‚  â† Feature quality assessment\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  8. Generate Final Report           â”‚  â† Metrics & comparison\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Prerequisites\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. **Generate Prompts**: Run `generate_retclip_prompts.ipynb` to create clinical descriptions\n",
    "2. **Upload CSV Files** to Google Drive:\n",
    "   - `retclip_prompts_full.csv` (12,989 train samples)\n",
    "   - `retclip_prompts_test.csv` (3,253 test samples)\n",
    "3. **HuggingFace Token**: Get from https://huggingface.co/settings/tokens\n",
    "4. **GPU**: Use Google Colab with A100 GPU (Runtime â†’ Change runtime type â†’ A100 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## â±ï¸ Estimated Runtime\n",
    "\n",
    "| Step | Duration (A100 GPU) |\n",
    "|------|--------------------| \n",
    "| Setup | ~5 minutes |\n",
    "| CSV â†’ TSV Conversion | ~2-3 hours |\n",
    "| LMDB Building | ~30 minutes |\n",
    "| RET-CLIP Training | ~6-8 hours |\n",
    "| Zero-Shot Evaluation | ~15 minutes |\n",
    "| Linear Probing | ~20 minutes |\n",
    "| **Total** | **~9-12 hours** |\n",
    "\n",
    "ğŸ’¾ **All artifacts are automatically saved to Google Drive throughout the pipeline.**\n",
    "\n",
    "---\n",
    "\n",
    "**Let's begin!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Setup & Environment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This step prepares the environment by:\n",
    "1. Checking GPU availability\n",
    "2. Mounting Google Drive for persistent storage\n",
    "3. Installing required Python packages\n",
    "4. Cloning the RET-CLIP repository\n",
    "5. Creating the folder structure\n",
    "\n",
    "## Required Packages\n",
    "\n",
    "- **ftfy, regex, tqdm**: Text processing and progress bars\n",
    "- **CLIP**: OpenAI's CLIP implementation (vision-language model base)\n",
    "- **transformers**: HuggingFace library for PubMedBERT\n",
    "- **lmdb**: Lightning Memory-Mapped Database for efficient data loading\n",
    "- **datasets, huggingface-hub**: Dataset downloading and authentication\n",
    "- **scikit-learn, matplotlib, seaborn**: Evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"âœ… Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Drive folder structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/RET-CLIP\"\n",
    "DRIVE_DATA = f\"{DRIVE_BASE}/data\"\n",
    "DRIVE_LMDB = f\"{DRIVE_BASE}/lmdb\"\n",
    "DRIVE_CHECKPOINTS = f\"{DRIVE_BASE}/checkpoints\"\n",
    "DRIVE_RESULTS = f\"{DRIVE_BASE}/results\"\n",
    "\n",
    "# Create directories\n",
    "for path in [DRIVE_BASE, DRIVE_DATA, DRIVE_LMDB, DRIVE_CHECKPOINTS, DRIVE_RESULTS]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Created Drive folder structure:\")\n",
    "print(f\"   {DRIVE_BASE}/\")\n",
    "print(f\"   â”œâ”€â”€ data/\")\n",
    "print(f\"   â”œâ”€â”€ lmdb/\")\n",
    "print(f\"   â”œâ”€â”€ checkpoints/\")\n",
    "print(f\"   â””â”€â”€ results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies...\\n\")\n",
    "\n",
    "!pip install -q ftfy regex tqdm\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "!pip install -q transformers\n",
    "!pip install -q lmdb\n",
    "!pip install -q datasets huggingface-hub\n",
    "!pip install -q scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"\\nâœ… All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ Important: RET-CLIP Repository Fixes\n",
    "\n",
    "The original RET-CLIP repository requires several fixes to work with English text and our dataset format. Before running this notebook, you need to apply these changes to your forked repository.\n",
    "\n",
    "### Required Fixes:\n",
    "\n",
    "#### 1. **Fix Chinese BERT â†’ English PubMedBERT Issue**\n",
    "\n",
    "**File**: `RET_CLIP/clip/model.py`\n",
    "\n",
    "**Problem**: The original code forces Chinese BERT tokenizer and config, breaking English models.\n",
    "\n",
    "**Fix**: In the `CLIP.__init__()` method, find this section:\n",
    "```python\n",
    "if isinstance(text_cfg, dict):\n",
    "    # Force Chinese config (WRONG!)\n",
    "    text_cfg = CLIPTextCfg(**text_cfg)\n",
    "```\n",
    "\n",
    "**Replace with**:\n",
    "```python\n",
    "if isinstance(text_cfg, dict):\n",
    "    # Keep the config as-is for any BERT model\n",
    "    text_cfg = CLIPTextCfg(**text_cfg)\n",
    "```\n",
    "\n",
    "Also ensure the tokenizer initialization uses the correct model name from config.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Fix Base64 Encoding (URL-safe)**\n",
    "\n",
    "**File**: `RET_CLIP/preprocess/build_lmdb_dataset_for_RET-CLIP.py`\n",
    "\n",
    "**Issue**: The LMDB builder must use URL-safe base64 decoding to match our encoding.\n",
    "\n",
    "**Current code** (line ~95):\n",
    "```python\n",
    "img_str = base64.b64decode(img_l)\n",
    "```\n",
    "\n",
    "**Fix**:\n",
    "```python\n",
    "img_str = base64.urlsafe_b64decode(img_l)\n",
    "```\n",
    "\n",
    "Apply this to both `img_l` and `img_r` decoding.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Fix TSV Format (3-column support)**\n",
    "\n",
    "**File**: `RET_CLIP/preprocess/build_lmdb_dataset_for_RET-CLIP.py`\n",
    "\n",
    "The code already expects 3 columns, but make sure it matches:\n",
    "```python\n",
    "patient_id, img_l, img_r = line.split(\"\\t\")\n",
    "```\n",
    "\n",
    "This is correct - our preprocessing creates 3-column TSV files.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Fix Model Loading (DDP compatibility)**\n",
    "\n",
    "**Files**: Evaluation scripts\n",
    "\n",
    "When loading checkpoints saved with DistributedDataParallel, strip the `module.` prefix:\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "# Remove 'module.' prefix from DDP\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace('module.', '')\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Add English BERT Model Configurations**\n",
    "\n",
    "**Directory**: `RET_CLIP/clip/model_configs/`\n",
    "\n",
    "**Issue**: The repository only includes Chinese BERT configs. We need to add English BERT model configs for our medical text.\n",
    "\n",
    "**Required config files** (create these JSON files):\n",
    "\n",
    "**a) `microsoft-BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext.json`** (Medical domain):\n",
    "```json\n",
    "{\n",
    "    \"vocab_size\": 30522,\n",
    "    \"text_attention_probs_dropout_prob\": 0.1,\n",
    "    \"text_hidden_act\": \"gelu\",\n",
    "    \"text_hidden_dropout_prob\": 0.1,\n",
    "    \"text_hidden_size\": 768,\n",
    "    \"text_initializer_range\": 0.02,\n",
    "    \"text_intermediate_size\": 3072,\n",
    "    \"text_max_position_embeddings\": 512,\n",
    "    \"text_num_attention_heads\": 12,\n",
    "    \"text_num_hidden_layers\": 12,\n",
    "    \"text_type_vocab_size\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "**b) `bert-base-uncased.json`** (General English):\n",
    "```json\n",
    "{\n",
    "    \"vocab_size\": 30522,\n",
    "    \"text_attention_probs_dropout_prob\": 0.1,\n",
    "    \"text_hidden_act\": \"gelu\",\n",
    "    \"text_hidden_dropout_prob\": 0.1,\n",
    "    \"text_hidden_size\": 768,\n",
    "    \"text_initializer_range\": 0.02,\n",
    "    \"text_intermediate_size\": 3072,\n",
    "    \"text_max_position_embeddings\": 512,\n",
    "    \"text_num_attention_heads\": 12,\n",
    "    \"text_num_hidden_layers\": 12,\n",
    "    \"text_type_vocab_size\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "**c) `dmis-lab-biobert-base-cased-v1.1.json`** (Biomedical domain):\n",
    "```json\n",
    "{\n",
    "    \"vocab_size\": 28996,\n",
    "    \"text_attention_probs_dropout_prob\": 0.1,\n",
    "    \"text_hidden_act\": \"gelu\",\n",
    "    \"text_hidden_dropout_prob\": 0.1,\n",
    "    \"text_hidden_size\": 768,\n",
    "    \"text_initializer_range\": 0.02,\n",
    "    \"text_intermediate_size\": 3072,\n",
    "    \"text_max_position_embeddings\": 512,\n",
    "    \"text_num_attention_heads\": 12,\n",
    "    \"text_num_hidden_layers\": 12,\n",
    "    \"text_type_vocab_size\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "**Where to add these**: Place all three JSON files in the `RET_CLIP/clip/model_configs/` directory of your forked repository.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Setup Guide:\n",
    "\n",
    "1. **Fork the RET-CLIP repository**: https://github.com/SUSTechBruce/RET-CLIP\n",
    "2. **Apply the fixes above** to your fork\n",
    "3. **Add the three BERT config files** to `RET_CLIP/clip/model_configs/`\n",
    "4. **Push to GitHub**\n",
    "5. **Update the clone command** in the next cell with your GitHub username\n",
    "\n",
    "Alternatively, you can use our pre-fixed repository (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone this repository (contains fixed RET-CLIP in retclip/)\nimport os\nimport shutil\n\n# This repository contains the fixed RET-CLIP in retclip/\nREPO_URL = \"https://github.com/FahadAlothman-fsd/retclip-english.git\"\n\nif not os.path.exists('/content/retclip_repo'):\n    print(f\"Cloning repository from {REPO_URL}...\")\n    !git clone {REPO_URL} /content/retclip_repo\n    print(\"âœ… Repository cloned\")\nelse:\n    print(\"âœ… Repository already exists\")\n\n# Copy retclip to /content/retclip for use in the pipeline\nif not os.path.exists('/content/retclip'):\n    print(\"\\nCopying fixed RET-CLIP from retclip...\")\n    shutil.copytree('/content/retclip_repo/retclip', '/content/retclip')\n    print(\"âœ… Copied to /content/retclip\")\nelse:\n    print(\"âœ… /content/retclip already exists\")\n\n# Add to Python path\nimport sys\nsys.path.insert(0, '/content/retclip')\n\nprint(\"\\nâœ… RET-CLIP repository ready\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"VERIFY: Using fixed RET-CLIP with:\")\nprint(\"=\"*80)\nprint(\"  1. âœ“ English BERT configs (PubMedBERT, BERT-base, BioBERT)\")\nprint(\"  2. âœ“ English BERT models in choices arrays\")\nprint(\"  3. âœ“ DDP checkpoint loading with 'module.' prefix stripping\")\nprint(\"  4. âœ“ URL-safe base64 encoding/decoding\")\nprint(\"  5. âœ“ 3-column TSV format support\")\nprint(\"\\nLocation: /content/retclip (copied from retclip/)\")\nprint(f\"Source: {REPO_URL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save setup log\n",
    "import datetime\n",
    "\n",
    "setup_log = f\"\"\"\n",
    "RET-CLIP Pipeline Setup Log\n",
    "===========================\n",
    "Date: {datetime.datetime.now()}\n",
    "GPU: Checked via nvidia-smi\n",
    "Drive: Mounted at /content/drive\n",
    "Repository: Cloned to /content/retclip\n",
    "Dependencies: Installed\n",
    "\n",
    "Drive Structure:\n",
    "{DRIVE_BASE}/\n",
    "â”œâ”€â”€ data/\n",
    "â”œâ”€â”€ lmdb/\n",
    "â”œâ”€â”€ checkpoints/\n",
    "â””â”€â”€ results/\n",
    "\n",
    "Status: âœ… Setup Complete\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{DRIVE_BASE}/setup_log.txt\", \"w\") as f:\n",
    "    f.write(setup_log)\n",
    "\n",
    "print(setup_log)\n",
    "print(f\"\\nâœ… Setup log saved to {DRIVE_BASE}/setup_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Load Pre-Generated Prompts\n",
    "\n",
    "Load the CSV files generated by `generate_retclip_prompts.ipynb`.\n",
    "\n",
    "**Important:** Make sure you've uploaded these files to your Google Drive first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# CONFIGURATION\n# ========================================\n\n# TEST MODE: Toggle for quick testing vs full pipeline\nTEST_MODE = True  # Set to False for full training\n\nif TEST_MODE:\n    TRAIN_SAMPLE_SIZE = 100  # Use 100 samples for testing\n    TEST_SAMPLE_SIZE = 50    # Use 50 samples for testing\n    NUM_EPOCHS_CONFIG = 2    # Train for only 2 epochs in test mode\n    print(\"âš ï¸ TEST MODE ENABLED - Using small sample for quick testing\")\nelse:\n    TRAIN_SAMPLE_SIZE = None  # Use all samples\n    TEST_SAMPLE_SIZE = None   # Use all samples\n    NUM_EPOCHS_CONFIG = 10    # Full training: 10 epochs\n    print(\"âœ… FULL MODE - Using complete dataset\")\n\n# TEXT ENCODER COMPARISON: Toggle to compare multiple text encoders\n# In TEST_MODE: ~1.5-2 hours total (3 encoders Ã— ~30 min each)\n# In FULL_MODE: ~18-24 hours total (3 encoders Ã— ~6-8 hours each)\nRUN_TEXT_ENCODER_COMPARISON = True  # Set to False to use only PubMedBERT\nprint(f\"Text Encoder Comparison: {'âœ… ENABLED' if RUN_TEXT_ENCODER_COMPARISON else 'âŒ DISABLED (single encoder only)'}\")\n\n# Configure text encoders to compare (if comparison enabled)\nTEXT_ENCODERS = [\n    {\n        \"name\": \"PubMedBERT\",\n        \"model_id\": \"microsoft-BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n        \"description\": \"Medical domain-specific (PubMed abstracts)\"\n    },\n    {\n        \"name\": \"BERT-base\",\n        \"model_id\": \"bert-base-uncased\",\n        \"description\": \"General English (Wikipedia + BookCorpus)\"\n    },\n    {\n        \"name\": \"BioBERT\",\n        \"model_id\": \"dmis-lab-biobert-base-cased-v1.1\",\n        \"description\": \"Biomedical domain (PubMed + PMC)\"\n    }\n]\n\nif RUN_TEXT_ENCODER_COMPARISON:\n    print(\"\\nğŸ“‹ Text Encoders to Compare:\")\n    for encoder in TEXT_ENCODERS:\n        print(f\"  â€¢ {encoder['name']}: {encoder['description']}\")\n\n# ========================================\n# SECRETS MANAGEMENT\n# ========================================\n\n# Try to load from Colab secrets first, fallback to manual entry\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"âœ… HuggingFace token loaded from Colab secrets\")\nexcept:\n    HF_TOKEN = \"\"  # Set manually if not using secrets\n    print(\"âš ï¸ HF_TOKEN not found in secrets - please set manually below or add to Colab secrets\")\n    print(\"   To add secrets: Click ğŸ”‘ in left sidebar â†’ Add HF_TOKEN\")\n\n# Manual override (leave empty if using secrets)\nHF_TOKEN_OVERRIDE = \"\"  # Optionally set your token here\nif HF_TOKEN_OVERRIDE:\n    HF_TOKEN = HF_TOKEN_OVERRIDE\n    print(\"âœ… Using manually set HF_TOKEN\")\n\n# ========================================\n# PATHS AND CONFIGURATION\n# ========================================\n\n# Load CSV files from cloned repo (faster than uploading to Drive!)\nTRAIN_CSV_PATH = \"/content/retclip_repo/content/retclip_prompts_full.csv\"\nTEST_CSV_PATH = \"/content/retclip_repo/content/retclip_prompts_test.csv\"\n\n# HuggingFace configuration\nHF_DATASET_NAME = \"Peacein/color-fundus-eye\"\n\n# Training configuration\nIMAGE_SIZE = 224\nCHECKPOINT_INTERVAL = 100  # Save progress every N samples\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CONFIGURATION SUMMARY\")\nprint(\"=\"*80)\nprint(f\"  Mode: {'TEST (small sample)' if TEST_MODE else 'FULL (complete dataset)'}\")\nif TEST_MODE:\n    print(f\"  Train samples: {TRAIN_SAMPLE_SIZE}\")\n    print(f\"  Test samples: {TEST_SAMPLE_SIZE}\")\n    print(f\"  Epochs: {NUM_EPOCHS_CONFIG}\")\nprint(f\"  Train CSV: {TRAIN_CSV_PATH}\")\nprint(f\"  Test CSV: {TEST_CSV_PATH}\")\nprint(f\"  CSV Source: Loaded from GitHub repo (no Drive upload needed!)\")\nprint(f\"  HuggingFace Dataset: {HF_DATASET_NAME}\")\nprint(f\"  Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\nprint(f\"  HF Token: {'âœ… Set' if HF_TOKEN else 'âŒ Not set - please update'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Load train CSV\nif not os.path.exists(TRAIN_CSV_PATH):\n    raise FileNotFoundError(f\"Train CSV not found at {TRAIN_CSV_PATH}. Please upload it to Google Drive.\")\n\ntrain_df = pd.read_csv(TRAIN_CSV_PATH)\nprint(f\"âœ… Loaded train CSV: {len(train_df)} samples (before sampling)\")\n\n# Apply TEST_MODE sampling if enabled\nif TEST_MODE and TRAIN_SAMPLE_SIZE:\n    train_df = train_df.head(TRAIN_SAMPLE_SIZE)\n    print(f\"âš ï¸ TEST MODE: Sampled {len(train_df)} training samples\")\n\nprint(f\"   Final size: {len(train_df)} samples\")\nprint(f\"   Columns: {train_df.columns.tolist()}\")\nprint(f\"\\nFirst 3 rows:\")\ndisplay(train_df.head(3))\n\n# Validate structure\nrequired_columns = ['dataset_index', 'label', 'prompt']\nfor col in required_columns:\n    if col not in train_df.columns:\n        raise ValueError(f\"Train CSV missing required column: {col}\")\n\nprint(f\"\\nâœ… Train CSV validation passed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load test CSV\nif not os.path.exists(TEST_CSV_PATH):\n    raise FileNotFoundError(f\"Test CSV not found at {TEST_CSV_PATH}. Please upload it to Google Drive.\")\n\ntest_df = pd.read_csv(TEST_CSV_PATH)\nprint(f\"âœ… Loaded test CSV: {len(test_df)} samples (before sampling)\")\n\n# Apply TEST_MODE sampling if enabled\nif TEST_MODE and TEST_SAMPLE_SIZE:\n    test_df = test_df.head(TEST_SAMPLE_SIZE)\n    print(f\"âš ï¸ TEST MODE: Sampled {len(test_df)} test samples\")\n\nprint(f\"   Final size: {len(test_df)} samples\")\nprint(f\"   Columns: {test_df.columns.tolist()}\")\nprint(f\"\\nFirst 3 rows:\")\ndisplay(test_df.head(3))\n\n# Validate structure\nfor col in required_columns:\n    if col not in test_df.columns:\n        raise ValueError(f\"Test CSV missing required column: {col}\")\n\nprint(f\"\\nâœ… Test CSV validation passed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  Total samples: {len(train_df)}\")\n",
    "print(f\"  Unique labels: {train_df['label'].nunique()}\")\n",
    "print(f\"  Label distribution:\")\n",
    "for label, count in train_df['label'].value_counts().items():\n",
    "    print(f\"    {label}: {count}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Total samples: {len(test_df)}\")\n",
    "print(f\"  Unique labels: {test_df['label'].nunique()}\")\n",
    "print(f\"  Label distribution:\")\n",
    "for label, count in test_df['label'].value_counts().items():\n",
    "    print(f\"    {label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Convert CSV â†’ TSV + JSONL\n",
    "\n",
    "Download images from HuggingFace, encode as base64, and create training files.\n",
    "\n",
    "**Output:**\n",
    "- TSV: `patient_id\\tleft_img_base64\\tright_img_base64` (3 columns)\n",
    "- JSONL: `{\"text_id\": 0, \"text\": \"prompt\", \"image_ids\": [\"patient_000000\"]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import time\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with HuggingFace\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"âœ… Authenticated with HuggingFace\")\n",
    "else:\n",
    "    print(\"âš ï¸ No HF_TOKEN set - download may fail\")\n",
    "\n",
    "def image_to_base64(image, size=224):\n",
    "    \"\"\"Convert PIL Image to URL-safe base64 string with resizing\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Resize image\n",
    "    image = image.resize((size, size), Image.BICUBIC)\n",
    "    \n",
    "    # Convert to base64 (URL-safe encoding)\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\", quality=95)\n",
    "    img_str = base64.urlsafe_b64encode(buffered.getvalue()).decode()\n",
    "    return img_str\n",
    "\n",
    "print(\"\\nâœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_tsv_jsonl(csv_df, split_name, output_dir, hf_dataset_name, image_size=224, checkpoint_interval=100):\n",
    "    \"\"\"\n",
    "    Convert CSV with prompts to TSV (images) + JSONL (text annotations)\n",
    "    \n",
    "    Args:\n",
    "        csv_df: DataFrame with columns [dataset_index, label, prompt]\n",
    "        split_name: 'train' or 'test'\n",
    "        output_dir: Where to save TSV and JSONL files\n",
    "        hf_dataset_name: HuggingFace dataset name\n",
    "        image_size: Resize images to this size\n",
    "        checkpoint_interval: Save checkpoint every N samples\n",
    "    \"\"\"\n",
    "    tsv_path = f\"{output_dir}/{split_name}_imgs.tsv\"\n",
    "    jsonl_path = f\"{output_dir}/{split_name}_texts.jsonl\"\n",
    "    checkpoint_path = f\"{output_dir}/{split_name}_checkpoint.json\"\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    processed_indices = set()\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            processed_indices = set(checkpoint.get('processed_indices', []))\n",
    "        print(f\"âœ… Resuming from checkpoint: {len(processed_indices)} samples already processed\")\n",
    "    \n",
    "    # Load HuggingFace dataset in streaming mode\n",
    "    print(f\"\\nLoading HuggingFace dataset: {hf_dataset_name} ({split_name})\")\n",
    "    hf_dataset = load_dataset(hf_dataset_name, split=split_name, streaming=True)\n",
    "    \n",
    "    # Open output files (append mode if resuming)\n",
    "    mode = 'a' if processed_indices else 'w'\n",
    "    tsv_file = open(tsv_path, mode, encoding='utf-8')\n",
    "    jsonl_file = open(jsonl_path, mode, encoding='utf-8')\n",
    "    \n",
    "    # Track unique patients to avoid duplicates\n",
    "    unique_patients_written = set()\n",
    "    if processed_indices and os.path.exists(tsv_path):\n",
    "        with open(tsv_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                patient_id = line.split('\\t')[0]\n",
    "                unique_patients_written.add(patient_id)\n",
    "    \n",
    "    print(f\"\\nProcessing {len(csv_df)} samples...\")\n",
    "    print(f\"Checkpoint interval: {checkpoint_interval}\")\n",
    "    \n",
    "    # Create iterator from streaming dataset\n",
    "    dataset_iter = iter(hf_dataset)\n",
    "    current_stream_index = 0\n",
    "    \n",
    "    # Process each row in CSV\n",
    "    for csv_idx, row in tqdm(csv_df.iterrows(), total=len(csv_df), desc=f\"Converting {split_name}\"):\n",
    "        dataset_index = int(row['dataset_index'])\n",
    "        label = row['label']\n",
    "        prompt = row['prompt']\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if dataset_index in processed_indices:\n",
    "            continue\n",
    "        \n",
    "        # Generate patient ID\n",
    "        patient_id = f\"patient_{dataset_index:06d}\"\n",
    "        \n",
    "        try:\n",
    "            # Stream forward to the correct index\n",
    "            while current_stream_index < dataset_index:\n",
    "                try:\n",
    "                    next(dataset_iter)\n",
    "                    current_stream_index += 1\n",
    "                except StopIteration:\n",
    "                    print(f\"\\nâš ï¸ Reached end of stream at index {current_stream_index}\")\n",
    "                    break\n",
    "            \n",
    "            # Get the sample at the correct index\n",
    "            if current_stream_index == dataset_index:\n",
    "                sample = next(dataset_iter)\n",
    "                current_stream_index += 1\n",
    "            else:\n",
    "                print(f\"âš ï¸ Could not reach index {dataset_index}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Extract and encode image\n",
    "            if 'image' in sample:\n",
    "                image = sample['image']\n",
    "                # For RET-CLIP binocular architecture, duplicate for both eyes\n",
    "                img_b64 = image_to_base64(image, image_size)\n",
    "                left_b64 = img_b64\n",
    "                right_b64 = img_b64  # Same image for both eyes\n",
    "            else:\n",
    "                print(f\"âš ï¸ No image field for index {dataset_index}\")\n",
    "                continue\n",
    "            \n",
    "            # Write to TSV (only if patient not already written)\n",
    "            if patient_id not in unique_patients_written:\n",
    "                tsv_file.write(f\"{patient_id}\\t{left_b64}\\t{right_b64}\\n\")\n",
    "                unique_patients_written.add(patient_id)\n",
    "            \n",
    "            # Write to JSONL\n",
    "            text_annotation = {\n",
    "                \"text_id\": csv_idx,\n",
    "                \"text\": prompt,\n",
    "                \"image_ids\": [patient_id]\n",
    "            }\n",
    "            jsonl_file.write(json.dumps(text_annotation, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            # Update checkpoint\n",
    "            processed_indices.add(dataset_index)\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (csv_idx + 1) % checkpoint_interval == 0:\n",
    "                with open(checkpoint_path, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'processed_indices': list(processed_indices),\n",
    "                        'last_index': dataset_index\n",
    "                    }, f)\n",
    "                tsv_file.flush()\n",
    "                jsonl_file.flush()\n",
    "            \n",
    "            # Small delay to avoid rate limits\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "        except StopIteration:\n",
    "            print(f\"\\nâš ï¸ Reached end of dataset stream at index {dataset_index}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error processing index {dataset_index}: {e}\")\n",
    "            # Save checkpoint on error\n",
    "            with open(checkpoint_path, 'w') as f:\n",
    "                json.dump({\n",
    "                    'processed_indices': list(processed_indices),\n",
    "                    'last_index': dataset_index\n",
    "                }, f)\n",
    "            tsv_file.flush()\n",
    "            jsonl_file.flush()\n",
    "            continue\n",
    "    \n",
    "    # Close files\n",
    "    tsv_file.close()\n",
    "    jsonl_file.close()\n",
    "    \n",
    "    # Clean up checkpoint after successful completion\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully created:\")\n",
    "    print(f\"   TSV: {tsv_path} ({len(unique_patients_written)} unique patients)\")\n",
    "    print(f\"   JSONL: {jsonl_path} ({len(processed_indices)} text-image pairs)\")\n",
    "    \n",
    "    return tsv_path, jsonl_path\n",
    "\n",
    "print(\"âœ… Conversion function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train set\n",
    "print(\"=\"*80)\n",
    "print(\"Converting TRAIN set to TSV + JSONL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_tsv, train_jsonl = convert_csv_to_tsv_jsonl(\n",
    "    csv_df=train_df,\n",
    "    split_name='train',\n",
    "    output_dir=DRIVE_DATA,\n",
    "    hf_dataset_name=HF_DATASET_NAME,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Converting TEST set to TSV + JSONL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_tsv, test_jsonl = convert_csv_to_tsv_jsonl(\n",
    "    csv_df=test_df,\n",
    "    split_name='test',\n",
    "    output_dir=DRIVE_DATA,\n",
    "    hf_dataset_name=HF_DATASET_NAME,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate TSV format\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Validating TSV files\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for tsv_path, split_name in [(train_tsv, 'train'), (test_tsv, 'test')]:\n",
    "    with open(tsv_path, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        parts = first_line.split('\\t')\n",
    "        \n",
    "        print(f\"\\n{split_name.upper()} TSV:\")\n",
    "        print(f\"  Columns: {len(parts)}\")\n",
    "        print(f\"  Expected: 3 (patient_id, left_img_b64, right_img_b64)\")\n",
    "        \n",
    "        if len(parts) == 3:\n",
    "            print(f\"  âœ… Format correct\")\n",
    "            print(f\"  Patient ID: {parts[0]}\")\n",
    "            print(f\"  Left image length: {len(parts[1])} chars\")\n",
    "            print(f\"  Right image length: {len(parts[2])} chars\")\n",
    "        else:\n",
    "            print(f\"  âŒ Format incorrect - expected 3 columns, got {len(parts)}\")\n",
    "            raise ValueError(f\"Invalid TSV format for {split_name}\")\n",
    "\n",
    "print(\"\\nâœ… All TSV files validated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Build LMDB Database\n",
    "\n",
    "Create LMDB databases for efficient data loading during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LMDB for train set\n",
    "print(\"=\"*80)\n",
    "print(\"Building LMDB for TRAIN set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python /content/retclip/RET_CLIP/preprocess/build_lmdb_dataset_for_RET-CLIP.py \\\n",
    "    --data_dir {DRIVE_DATA} \\\n",
    "    --splits train \\\n",
    "    --lmdb_dir {DRIVE_LMDB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LMDB for test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Building LMDB for TEST set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python /content/retclip/RET_CLIP/preprocess/build_lmdb_dataset_for_RET-CLIP.py \\\n",
    "    --data_dir {DRIVE_DATA} \\\n",
    "    --splits test \\\n",
    "    --lmdb_dir {DRIVE_LMDB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate LMDB by reading a few samples\n",
    "import lmdb\n",
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Validating LMDB databases\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name in ['train', 'test']:\n",
    "    lmdb_path = f\"{DRIVE_LMDB}/{split_name}/imgs\"\n",
    "    \n",
    "    if not os.path.exists(lmdb_path):\n",
    "        print(f\"âŒ LMDB not found: {lmdb_path}\")\n",
    "        continue\n",
    "    \n",
    "    env = lmdb.open(lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        # Try to read first 3 samples\n",
    "        print(f\"\\n{split_name.upper()} LMDB:\")\n",
    "        for i in range(3):\n",
    "            patient_id = f\"patient_{i:06d}\"\n",
    "            value = txn.get(patient_id.encode('utf-8'))\n",
    "            \n",
    "            if value is None:\n",
    "                print(f\"  âš ï¸ No data for {patient_id}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                img_left_b64, img_right_b64 = pickle.loads(value)\n",
    "                print(f\"  âœ… {patient_id}: left={len(img_left_b64)} chars, right={len(img_right_b64)} chars\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error unpacking {patient_id}: {e}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(\"\\nâœ… LMDB validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Train RET-CLIP (10 Epochs)\n",
    "\n",
    "Train RET-CLIP using contrastive learning on fundus images and clinical prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\n# âš ï¸ IMPORTANT: Batch size adjusted for T4 GPU (14.74 GiB VRAM)\n# For T4: Use batch size 32-64 (128 is too large and causes OOM)\n# For A100: Can use batch size 128-256\nBATCH_SIZE = 32  # Reduced from 128 for T4 GPU compatibility\nNUM_EPOCHS = NUM_EPOCHS_CONFIG  # Use config from cell-10 (2 for TEST_MODE, 10 for FULL_MODE)\nLEARNING_RATE = 1e-5\nWARMUP_STEPS = 500\nVISION_MODEL = \"ViT-B-16\"\nTEXT_MODEL = \"microsoft-BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n\nprint(\"Training Configuration:\")\nprint(f\"  Mode: {'TEST' if TEST_MODE else 'FULL'}\")\nprint(f\"  GPU: Tesla T4 (14.74 GiB VRAM)\")\nprint(f\"  Batch size: {BATCH_SIZE} (optimized for T4)\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Warmup steps: {WARMUP_STEPS}\")\nprint(f\"  Vision model: {VISION_MODEL}\")\nprint(f\"  Text model: {TEXT_MODEL}\")\nprint(f\"  Train samples: {len(train_df)}\")\nprint(f\"  LMDB dir: {DRIVE_LMDB}/train\")\nprint(f\"  Checkpoint dir: {DRIVE_CHECKPOINTS}\")\nprint(f\"\\nğŸ’¡ Note: Batch size 32 prevents OOM on T4. Training will be slower but stable.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set PYTHONPATH for subprocess\nimport os\nos.environ['PYTHONPATH'] = '/content/retclip'\nprint(f\"âœ… Set PYTHONPATH to: {os.environ['PYTHONPATH']}\")\n\n# Run training with distributed launcher\nprint(\"\\n\" + \"=\"*80)\nprint(\"Starting RET-CLIP Training\")\nprint(\"=\"*80)\nprint(f\"\\nThis will take approximately 6-8 hours on A100 GPU...\\n\")\n\n!torchrun --nproc_per_node=1 --master_port=29500 \\\n    /content/retclip/RET_CLIP/training/main.py \\\n    --train-data {DRIVE_LMDB}/train \\\n    --batch-size {BATCH_SIZE} \\\n    --max-epochs {NUM_EPOCHS} \\\n    --lr {LEARNING_RATE} \\\n    --warmup {WARMUP_STEPS} \\\n    --vision-model {VISION_MODEL} \\\n    --text-model {TEXT_MODEL} \\\n    --logs {DRIVE_CHECKPOINTS} \\\n    --name retclip_fundus \\\n    --save-epoch-frequency 1 \\\n    --skip-aggregate"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List saved checkpoints\nprint(\"\\n\" + \"=\"*80)\nprint(\"Saved Checkpoints\")\nprint(\"=\"*80)\n\n# The training script creates a subdirectory structure: logs/name/checkpoints/\ncheckpoint_dir = f\"{DRIVE_CHECKPOINTS}/retclip_fundus/checkpoints\"\n\nif os.path.exists(checkpoint_dir):\n    checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')])\n    \n    if checkpoint_files:\n        print(f\"\\nâœ… Found {len(checkpoint_files)} checkpoint(s) in {checkpoint_dir}:\")\n        for ckpt in checkpoint_files:\n            ckpt_path = f\"{checkpoint_dir}/{ckpt}\"\n            size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n            print(f\"  {ckpt} ({size_mb:.1f} MB)\")\n    else:\n        print(f\"âŒ No .pt files found in {checkpoint_dir}\")\n    \n    # Verify final checkpoint exists (epoch2.pt for TEST_MODE, epoch_10.pt for FULL_MODE)\n    final_checkpoint = f\"{checkpoint_dir}/epoch{NUM_EPOCHS}.pt\"\n    if os.path.exists(final_checkpoint):\n        print(f\"\\nâœ… Final checkpoint ready: epoch{NUM_EPOCHS}.pt\")\n        print(f\"   Path: {final_checkpoint}\")\n    else:\n        print(f\"\\nâš ï¸ Final checkpoint not found: epoch{NUM_EPOCHS}.pt\")\n        print(f\"   Expected: {final_checkpoint}\")\nelse:\n    print(f\"âŒ Checkpoint directory not found: {checkpoint_dir}\")\n    print(f\"\\nListing all files in {DRIVE_CHECKPOINTS}:\")\n    for item in os.listdir(DRIVE_CHECKPOINTS):\n        print(f\"  {item}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5B: Text Encoder Comparison (Optional)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Compare different text encoders to evaluate which performs best for retinal disease classification:\n",
    "\n",
    "1. **PubMedBERT** (medical domain-specific) - trained on PubMed abstracts\n",
    "2. **Standard BERT** (general English) - trained on Wikipedia + BookCorpus  \n",
    "3. **BioBERT** (biomedical domain) - BERT fine-tuned on PubMed + PMC articles\n",
    "\n",
    "This allows us to quantify the benefit of domain-specific language models for medical image-text alignment.\n",
    "\n",
    "**Note**: This step is optional but recommended for research purposes. It will train 3 separate RET-CLIP models (one for each text encoder), which will take ~18-24 hours total on A100 GPU.\n",
    "\n",
    "If you want to skip this comparison and only use PubMedBERT, jump to Step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set PYTHONPATH for subprocess\nimport os\nos.environ['PYTHONPATH'] = '/content/retclip'\n\n# Train RET-CLIP with each text encoder (if comparison enabled)\nif RUN_TEXT_ENCODER_COMPARISON:\n    print(\"\\n\" + \"=\"*80)\n    print(\"Training RET-CLIP with Multiple Text Encoders\")\n    print(\"=\"*80)\n    \n    encoder_results = {}\n    \n    for encoder_config in TEXT_ENCODERS:\n        encoder_name = encoder_config['name']\n        encoder_model_id = encoder_config['model_id']\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"Training with {encoder_name}\")\n        print(f\"{'='*80}\")\n        print(f\"Model ID: {encoder_model_id}\")\n        print(f\"Description: {encoder_config['description']}\")\n        print(f\"\\nEstimated time: ~6-8 hours on A100 GPU\")\n        print(f\"Starting training...\\n\")\n        \n        # Create checkpoint directory for this encoder\n        encoder_checkpoint_dir = f\"{DRIVE_CHECKPOINTS}/{encoder_name.lower().replace('-', '_')}\"\n        os.makedirs(encoder_checkpoint_dir, exist_ok=True)\n        \n        # Run training with distributed launcher\n        !torchrun --nproc_per_node=1 --master_port=29500 \\\n            /content/retclip/RET_CLIP/training/main.py \\\n            --train-data {DRIVE_LMDB}/train \\\n            --batch-size {BATCH_SIZE} \\\n            --max-epochs {NUM_EPOCHS} \\\n            --lr {LEARNING_RATE} \\\n            --warmup {WARMUP_STEPS} \\\n            --vision-model {VISION_MODEL} \\\n            --text-model {encoder_model_id} \\\n            --logs {encoder_checkpoint_dir} \\\n            --name retclip_{encoder_name.lower()} \\\n            --save-epoch-frequency 1 \\\n            --skip-aggregate\n        \n        # Store checkpoint path (with correct subdirectory structure)\n        final_checkpoint = f\"{encoder_checkpoint_dir}/retclip_{encoder_name.lower()}/checkpoints/epoch{NUM_EPOCHS}.pt\"\n        if os.path.exists(final_checkpoint):\n            encoder_results[encoder_name] = {\n                'checkpoint': final_checkpoint,\n                'model_id': encoder_model_id,\n                'status': 'success'\n            }\n            print(f\"\\nâœ… {encoder_name} training complete\")\n            print(f\"   Checkpoint: {final_checkpoint}\")\n        else:\n            encoder_results[encoder_name] = {\n                'checkpoint': None,\n                'model_id': encoder_model_id,\n                'status': 'failed'\n            }\n            print(f\"\\nâŒ {encoder_name} training failed - checkpoint not found\")\n            print(f\"   Expected: {final_checkpoint}\")\n    \n    # Save encoder results\n    import json\n    with open(f\"{DRIVE_RESULTS}/encoder_training_results.json\", 'w') as f:\n        json.dump(encoder_results, f, indent=2)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"All encoder training complete!\")\n    print(\"=\"*80)\n    \n    for encoder_name, result in encoder_results.items():\n        status_icon = \"âœ…\" if result['status'] == 'success' else \"âŒ\"\n        print(f\"{status_icon} {encoder_name}: {result['status']}\")\n        \nelse:\n    print(\"\\nâš ï¸ Skipping text encoder comparison training\")\n    print(\"   Only using PubMedBERT model from Step 5\")\n    \n    # Store the single encoder result for later evaluation (with correct path structure)\n    encoder_results = {\n        \"PubMedBERT\": {\n            'checkpoint': f\"{DRIVE_CHECKPOINTS}/retclip_fundus/checkpoints/epoch{NUM_EPOCHS}.pt\",\n            'model_id': TEXT_MODEL,\n            'status': 'success'\n        }\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate All Text Encoders\n",
    "\n",
    "Perform zero-shot evaluation on all trained text encoders to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# NOTE: This cell should be run AFTER cell-29 (which creates encoder_results)\n# Check if encoder_results exists (created by cell-29)\ntry:\n    encoder_results\nexcept NameError:\n    print(\"âš ï¸ encoder_results not found - please run cell-29 first to train encoders\")\n    print(\"   Skipping encoder comparison evaluation\")\n    encoder_results = {}\n\n# Evaluate all text encoders (if comparison enabled)\nif RUN_TEXT_ENCODER_COMPARISON and len(encoder_results) > 0:\n    print(\"\\n\" + \"=\"*80)\n    print(\"Evaluating All Text Encoders (Zero-Shot)\")\n    print(\"=\"*80)\n    \n    encoder_comparison_metrics = {}\n    \n    for encoder_name, encoder_info in encoder_results.items():\n        if encoder_info['status'] != 'success':\n            print(f\"\\nâš ï¸ Skipping {encoder_name} - training failed\")\n            continue\n            \n        print(f\"\\n{'='*80}\")\n        print(f\"Evaluating {encoder_name}\")\n        print(f\"{'='*80}\")\n        print(f\"Model: {encoder_info['model_id']}\")\n        print(f\"Checkpoint: {encoder_info['checkpoint']}\")\n        \n        # Load model\n        from RET_CLIP.clip.model import CLIP\n        import json\n        \n        vision_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{VISION_MODEL}.json\"\n        text_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{encoder_info['model_id']}.json\"\n        \n        with open(vision_config_path, 'r') as fv, open(text_config_path, 'r') as ft:\n            model_cfg = json.load(fv)\n            for k, v in json.load(ft).items():\n                model_cfg[k] = v\n        \n        # Initialize model\n        model = CLIP(**model_cfg)\n        \n        # Load checkpoint\n        import torch\n        checkpoint = torch.load(encoder_info['checkpoint'], map_location='cpu')\n        state_dict = checkpoint['state_dict']\n        \n        # Handle DDP\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            name = k.replace('module.', '')\n            new_state_dict[name] = v\n        \n        model.load_state_dict(new_state_dict)\n        model = model.cuda()\n        model.eval()\n        \n        print(f\"âœ… Model loaded\")\n        \n        # Prepare prompts\n        disease_classes = test_df['label'].unique().tolist()\n        zero_shot_prompts = {}\n        for disease in disease_classes:\n            sample_prompt = test_df[test_df['label'] == disease]['prompt'].iloc[0]\n            zero_shot_prompts[disease] = sample_prompt\n        \n        # Encode text\n        from RET_CLIP.clip import tokenize\n        \n        disease_list = list(zero_shot_prompts.keys())\n        prompt_list = [zero_shot_prompts[d] for d in disease_list]\n        \n        text_tokens = tokenize(prompt_list).cuda()\n        \n        with torch.no_grad():\n            text_features = model.encode_text(text_tokens)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        \n        print(f\"âœ… Text features encoded: {text_features.shape}\")\n        \n        # Image preprocessing\n        from torchvision import transforms\n        from PIL import Image\n        from io import BytesIO\n        import base64\n        import pickle\n        import lmdb\n        from tqdm.notebook import tqdm\n        \n        transform = transforms.Compose([\n            transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.CenterCrop(IMAGE_SIZE),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711)\n            )\n        ])\n        \n        # Zero-shot inference\n        test_lmdb_path = f\"{DRIVE_LMDB}/test\"\n        env = lmdb.open(test_lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n        \n        predictions = []\n        ground_truth = []\n        \n        with env.begin() as txn:\n            for idx in tqdm(range(len(test_df)), desc=f\"  {encoder_name} inference\"):\n                try:\n                    patient_id = f\"patient_{idx:06d}\"\n                    value = txn.get(patient_id.encode('utf-8'))\n                    \n                    if value is None:\n                        continue\n                    \n                    img_left_b64, img_right_b64 = pickle.loads(value)\n                    img_bytes = base64.urlsafe_b64decode(img_left_b64)\n                    image = Image.open(BytesIO(img_bytes)).convert('RGB')\n                    \n                    img_tensor = transform(image).unsqueeze(0).cuda()\n                    \n                    with torch.no_grad():\n                        img_features = model.encode_image(img_tensor)\n                        img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n                    \n                    similarities = (img_features @ text_features.T).squeeze(0)\n                    pred_idx = similarities.argmax().item()\n                    predicted_disease = disease_list[pred_idx]\n                    \n                    true_disease = test_df.iloc[idx]['label']\n                    \n                    predictions.append(predicted_disease)\n                    ground_truth.append(true_disease)\n                    \n                except Exception as e:\n                    continue\n        \n        env.close()\n        \n        # Compute metrics\n        from sklearn.metrics import accuracy_score, classification_report, f1_score\n        \n        accuracy = accuracy_score(ground_truth, predictions)\n        macro_f1 = f1_score(ground_truth, predictions, average='macro')\n        weighted_f1 = f1_score(ground_truth, predictions, average='weighted')\n        \n        report = classification_report(ground_truth, predictions, \n                                      target_names=disease_list, output_dict=True)\n        \n        encoder_comparison_metrics[encoder_name] = {\n            'accuracy': float(accuracy),\n            'macro_f1': float(macro_f1),\n            'weighted_f1': float(weighted_f1),\n            'per_class': report\n        }\n        \n        print(f\"\\nâœ… {encoder_name} Results:\")\n        print(f\"   Accuracy: {accuracy * 100:.2f}%\")\n        print(f\"   Macro F1: {macro_f1 * 100:.2f}%\")\n        print(f\"   Weighted F1: {weighted_f1 * 100:.2f}%\")\n        \n        # Clean up\n        del model\n        torch.cuda.empty_cache()\n    \n    # Save comparison metrics\n    import json\n    with open(f\"{DRIVE_RESULTS}/encoder_comparison_metrics.json\", 'w') as f:\n        json.dump(encoder_comparison_metrics, f, indent=2)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"Text Encoder Comparison Summary\")\n    print(\"=\"*80)\n    \n    # Create comparison table\n    import pandas as pd\n    comparison_data = []\n    for encoder_name, metrics in encoder_comparison_metrics.items():\n        comparison_data.append({\n            'Text Encoder': encoder_name,\n            'Accuracy': f\"{metrics['accuracy'] * 100:.2f}%\",\n            'Macro F1': f\"{metrics['macro_f1'] * 100:.2f}%\",\n            'Weighted F1': f\"{metrics['weighted_f1'] * 100:.2f}%\"\n        })\n    \n    comparison_df = pd.DataFrame(comparison_data)\n    display(comparison_df)\n    \n    # Save comparison\n    comparison_df.to_csv(f\"{DRIVE_RESULTS}/encoder_comparison_table.csv\", index=False)\n    \n    print(f\"\\nâœ… Comparison saved to {DRIVE_RESULTS}/encoder_comparison_table.csv\")\n    \n    # Determine best encoder\n    best_encoder = max(encoder_comparison_metrics.items(), \n                      key=lambda x: x[1]['accuracy'])\n    print(f\"\\nğŸ† Best Text Encoder: {best_encoder[0]}\")\n    print(f\"   Accuracy: {best_encoder[1]['accuracy'] * 100:.2f}%\")\n    \nelse:\n    if not RUN_TEXT_ENCODER_COMPARISON:\n        print(\"\\nâš ï¸ Text encoder comparison disabled\")\n        print(f\"   Set RUN_TEXT_ENCODER_COMPARISON = True in cell-10 to enable\")\n    else:\n        print(\"\\nâš ï¸ No encoder results available - run cell-29 first\")\n        print(f\"   Using single encoder mode (PubMedBERT only)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Zero-Shot Evaluation\n",
    "\n",
    "Test RET-CLIP's vision-language alignment by computing similarity between image embeddings and text embeddings for all disease classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n\n# Load RET-CLIP model\nprint(\"Loading RET-CLIP model...\")\n\nfrom RET_CLIP.clip.model import CLIP\n\n# Load model configs\nvision_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{VISION_MODEL}.json\"\ntext_config_path = f\"/content/retclip/RET_CLIP/clip/model_configs/{TEXT_MODEL}.json\"\n\nwith open(vision_config_path, 'r') as fv, open(text_config_path, 'r') as ft:\n    model_cfg = json.load(fv)\n    for k, v in json.load(ft).items():\n        model_cfg[k] = v\n\n# Initialize model\nmodel = CLIP(**model_cfg)\n\n# Load trained checkpoint (uses correct path based on training structure)\ncheckpoint_path = f\"{DRIVE_CHECKPOINTS}/retclip_fundus/checkpoints/epoch{NUM_EPOCHS}.pt\"\nprint(f\"Loading checkpoint from: {checkpoint_path}\")\n\ncheckpoint = torch.load(checkpoint_path, map_location='cpu')\n\n# Handle DDP state dict (strip 'module.' prefix)\nstate_dict = checkpoint['state_dict']\nnew_state_dict = {}\nfor k, v in state_dict.items():\n    name = k.replace('module.', '')\n    new_state_dict[name] = v\n\nmodel.load_state_dict(new_state_dict)\nmodel = model.cuda()\nmodel.eval()\n\nprint(\"âœ… Model loaded successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare zero-shot prompts for all disease classes\n",
    "# Use diverse prompts from the test set for each class\n",
    "\n",
    "disease_classes = test_df['label'].unique().tolist()\n",
    "print(f\"Disease classes: {len(disease_classes)}\")\n",
    "print(disease_classes)\n",
    "\n",
    "# Get one representative prompt per disease class\n",
    "zero_shot_prompts = {}\n",
    "for disease in disease_classes:\n",
    "    # Get a random prompt for this disease\n",
    "    sample_prompt = test_df[test_df['label'] == disease]['prompt'].iloc[0]\n",
    "    zero_shot_prompts[disease] = sample_prompt\n",
    "\n",
    "print(\"\\nZero-shot prompts:\")\n",
    "for disease, prompt in zero_shot_prompts.items():\n",
    "    print(f\"  {disease}: {prompt[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text prompts\n",
    "from RET_CLIP.clip import tokenize\n",
    "\n",
    "print(\"\\nEncoding text prompts...\")\n",
    "\n",
    "disease_list = list(zero_shot_prompts.keys())\n",
    "prompt_list = [zero_shot_prompts[d] for d in disease_list]\n",
    "\n",
    "# Tokenize and encode\n",
    "text_tokens = tokenize(prompt_list).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"âœ… Text features shape: {text_features.shape}\")\n",
    "print(f\"   {len(disease_list)} disease classes encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract image features from test set and perform zero-shot classification\nfrom torchvision import transforms\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\nimport pickle\nimport lmdb\nfrom tqdm.notebook import tqdm\n\n# Image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=(0.48145466, 0.4578275, 0.40821073),\n        std=(0.26862954, 0.26130258, 0.27577711)\n    )\n])\n\n# Load test LMDB\ntest_lmdb_path = f\"{DRIVE_LMDB}/test\"\nenv = lmdb.open(test_lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n\nprint(\"\\nPerforming zero-shot classification on test set...\")\n\npredictions = []\nground_truth = []\n\nwith env.begin() as txn:\n    for idx in tqdm(range(len(test_df)), desc=\"Zero-shot inference\"):\n        try:\n            # Get image from LMDB\n            patient_id = f\"patient_{idx:06d}\"\n            value = txn.get(patient_id.encode('utf-8'))\n            \n            if value is None:\n                continue\n            \n            # Decode image\n            img_left_b64, img_right_b64 = pickle.loads(value)\n            img_bytes = base64.urlsafe_b64decode(img_left_b64)\n            image = Image.open(BytesIO(img_bytes)).convert('RGB')\n            \n            # Transform and encode image\n            img_tensor = transform(image).unsqueeze(0).cuda()\n            \n            with torch.no_grad():\n                img_features = model.encode_image(img_tensor)\n                img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n            \n            # Compute similarity with all disease prompts\n            similarities = (img_features @ text_features.T).squeeze(0)\n            \n            # Predict class\n            pred_idx = similarities.argmax().item()\n            predicted_disease = disease_list[pred_idx]\n            \n            # Get ground truth\n            true_disease = test_df.iloc[idx]['label']\n            \n            predictions.append(predicted_disease)\n            ground_truth.append(true_disease)\n            \n        except Exception as e:\n            print(f\"Error at index {idx}: {e}\")\n            continue\n\nenv.close()\n\nprint(f\"\\nâœ… Zero-shot inference complete\")\nprint(f\"   Processed {len(predictions)} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute zero-shot metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ZERO-SHOT EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Accuracy\n",
    "zeroshot_acc = accuracy_score(ground_truth, predictions)\n",
    "print(f\"\\nZero-Shot Accuracy: {zeroshot_acc * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "report = classification_report(ground_truth, predictions, target_names=disease_list, output_dict=True)\n",
    "print(classification_report(ground_truth, predictions, target_names=disease_list))\n",
    "\n",
    "# Save metrics\n",
    "zeroshot_metrics = {\n",
    "    'accuracy': float(zeroshot_acc),\n",
    "    'per_class': report\n",
    "}\n",
    "\n",
    "with open(f\"{DRIVE_RESULTS}/zeroshot_metrics.json\", 'w') as f:\n",
    "    json.dump(zeroshot_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Metrics saved to {DRIVE_RESULTS}/zeroshot_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(ground_truth, predictions, labels=disease_list)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=disease_list, yticklabels=disease_list)\n",
    "plt.title('Zero-Shot Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_RESULTS}/zeroshot_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Confusion matrix saved to {DRIVE_RESULTS}/zeroshot_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: Linear Probing Evaluation\n",
    "\n",
    "Train a linear classifier on frozen RET-CLIP features to evaluate feature quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from train set\n",
    "print(\"=\"*80)\n",
    "print(\"Extracting features from TRAIN set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_lmdb_path = f\"{DRIVE_LMDB}/train\"\n",
    "env = lmdb.open(train_lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "# Create label mapping\n",
    "label_to_idx = {label: idx for idx, label in enumerate(disease_list)}\n",
    "\n",
    "with env.begin() as txn:\n",
    "    for idx in tqdm(range(len(train_df)), desc=\"Extracting train features\"):\n",
    "        try:\n",
    "            patient_id = f\"patient_{idx:06d}\"\n",
    "            value = txn.get(patient_id.encode('utf-8'))\n",
    "            \n",
    "            if value is None:\n",
    "                continue\n",
    "            \n",
    "            # Decode image\n",
    "            img_left_b64, img_right_b64 = pickle.loads(value)\n",
    "            img_bytes = base64.urlsafe_b64decode(img_left_b64)\n",
    "            image = Image.open(BytesIO(img_bytes)).convert('RGB')\n",
    "            \n",
    "            # Transform and extract features\n",
    "            img_tensor = transform(image).unsqueeze(0).cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                img_features = model.encode_image(img_tensor)\n",
    "                img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            train_features.append(img_features.cpu().numpy())\n",
    "            \n",
    "            # Get label\n",
    "            label = train_df.iloc[idx]['label']\n",
    "            train_labels.append(label_to_idx[label])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "env.close()\n",
    "\n",
    "train_features = np.vstack(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "print(f\"\\nâœ… Extracted train features: {train_features.shape}\")\n",
    "print(f\"   Labels: {train_labels.shape}\")\n",
    "\n",
    "# Save features\n",
    "np.save(f\"{DRIVE_RESULTS}/train_features.npy\", train_features)\n",
    "np.save(f\"{DRIVE_RESULTS}/train_labels.npy\", train_labels)\n",
    "print(f\"   Saved to {DRIVE_RESULTS}/train_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Extracting features from TEST set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_lmdb_path = f\"{DRIVE_LMDB}/test\"\n",
    "env = lmdb.open(test_lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "with env.begin() as txn:\n",
    "    for idx in tqdm(range(len(test_df)), desc=\"Extracting test features\"):\n",
    "        try:\n",
    "            patient_id = f\"patient_{idx:06d}\"\n",
    "            value = txn.get(patient_id.encode('utf-8'))\n",
    "            \n",
    "            if value is None:\n",
    "                continue\n",
    "            \n",
    "            # Decode image\n",
    "            img_left_b64, img_right_b64 = pickle.loads(value)\n",
    "            img_bytes = base64.urlsafe_b64decode(img_left_b64)\n",
    "            image = Image.open(BytesIO(img_bytes)).convert('RGB')\n",
    "            \n",
    "            # Transform and extract features\n",
    "            img_tensor = transform(image).unsqueeze(0).cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                img_features = model.encode_image(img_tensor)\n",
    "                img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            test_features.append(img_features.cpu().numpy())\n",
    "            \n",
    "            # Get label\n",
    "            label = test_df.iloc[idx]['label']\n",
    "            test_labels.append(label_to_idx[label])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "env.close()\n",
    "\n",
    "test_features = np.vstack(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"\\nâœ… Extracted test features: {test_features.shape}\")\n",
    "print(f\"   Labels: {test_labels.shape}\")\n",
    "\n",
    "# Save features\n",
    "np.save(f\"{DRIVE_RESULTS}/test_features.npy\", test_features)\n",
    "np.save(f\"{DRIVE_RESULTS}/test_labels.npy\", test_labels)\n",
    "print(f\"   Saved to {DRIVE_RESULTS}/test_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle as pkl\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Linear Probe Classifier\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train classifier\n",
    "classifier = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "print(\"\\nâœ… Classifier trained successfully\")\n",
    "\n",
    "# Save classifier\n",
    "with open(f\"{DRIVE_RESULTS}/linear_probe_classifier.pkl\", 'wb') as f:\n",
    "    pkl.dump(classifier, f)\n",
    "\n",
    "print(f\"   Saved to {DRIVE_RESULTS}/linear_probe_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classifier\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LINEAR PROBING EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predict on test set\n",
    "linear_probe_preds = classifier.predict(test_features)\n",
    "\n",
    "# Accuracy\n",
    "linear_probe_acc = accuracy_score(test_labels, linear_probe_preds)\n",
    "print(f\"\\nLinear Probe Accuracy: {linear_probe_acc * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "linear_report = classification_report(test_labels, linear_probe_preds, \n",
    "                                     target_names=disease_list, output_dict=True)\n",
    "print(classification_report(test_labels, linear_probe_preds, target_names=disease_list))\n",
    "\n",
    "# Save metrics\n",
    "linear_probe_metrics = {\n",
    "    'accuracy': float(linear_probe_acc),\n",
    "    'per_class': linear_report\n",
    "}\n",
    "\n",
    "with open(f\"{DRIVE_RESULTS}/linear_probe_metrics.json\", 'w') as f:\n",
    "    json.dump(linear_probe_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Metrics saved to {DRIVE_RESULTS}/linear_probe_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(test_labels, linear_probe_preds)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=disease_list, yticklabels=disease_list)\n",
    "plt.title('Linear Probing Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_RESULTS}/linear_probe_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Confusion matrix saved to {DRIVE_RESULTS}/linear_probe_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8: Generate Final Report\n",
    "\n",
    "Consolidate all metrics and create comparison report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Zero-Shot', 'Linear Probing'],\n",
    "    'Accuracy': [\n",
    "        f\"{zeroshot_acc * 100:.2f}%\",\n",
    "        f\"{linear_probe_acc * 100:.2f}%\"\n",
    "    ],\n",
    "    'Macro F1': [\n",
    "        f\"{report['macro avg']['f1-score'] * 100:.2f}%\",\n",
    "        f\"{linear_report['macro avg']['f1-score'] * 100:.2f}%\"\n",
    "    ],\n",
    "    'Weighted F1': [\n",
    "        f\"{report['weighted avg']['f1-score'] * 100:.2f}%\",\n",
    "        f\"{linear_report['weighted avg']['f1-score'] * 100:.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(f\"{DRIVE_RESULTS}/metrics_comparison.csv\", index=False)\n",
    "print(f\"\\nâœ… Saved to {DRIVE_RESULTS}/metrics_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "final_report = f\"\"\"\n",
    "{'='*80}\n",
    "RET-CLIP TRAINING & EVALUATION REPORT\n",
    "{'='*80}\n",
    "\n",
    "Date: {datetime.datetime.now()}\n",
    "\n",
    "DATASET STATISTICS\n",
    "{'-'*80}\n",
    "Train samples: {len(train_df)}\n",
    "Test samples: {len(test_df)}\n",
    "Disease classes: {len(disease_list)}\n",
    "Classes: {', '.join(disease_list)}\n",
    "\n",
    "TRAINING CONFIGURATION\n",
    "{'-'*80}\n",
    "Vision Model: {VISION_MODEL}\n",
    "Text Model: {TEXT_MODEL}\n",
    "Batch Size: {BATCH_SIZE}\n",
    "Epochs: {NUM_EPOCHS}\n",
    "Learning Rate: {LEARNING_RATE}\n",
    "Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\n",
    "\n",
    "EVALUATION RESULTS\n",
    "{'-'*80}\n",
    "\n",
    "1. Zero-Shot Evaluation (Vision-Language Alignment)\n",
    "   Accuracy: {zeroshot_acc * 100:.2f}%\n",
    "   Macro F1: {report['macro avg']['f1-score'] * 100:.2f}%\n",
    "   Weighted F1: {report['weighted avg']['f1-score'] * 100:.2f}%\n",
    "\n",
    "2. Linear Probing (Feature Quality)\n",
    "   Accuracy: {linear_probe_acc * 100:.2f}%\n",
    "   Macro F1: {linear_report['macro avg']['f1-score'] * 100:.2f}%\n",
    "   Weighted F1: {linear_report['weighted avg']['f1-score'] * 100:.2f}%\n",
    "\n",
    "ARTIFACTS SAVED TO GOOGLE DRIVE\n",
    "{'-'*80}\n",
    "{DRIVE_BASE}/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ train_imgs.tsv\n",
    "â”‚   â”œâ”€â”€ test_imgs.tsv\n",
    "â”‚   â”œâ”€â”€ train_texts.jsonl\n",
    "â”‚   â””â”€â”€ test_texts.jsonl\n",
    "â”œâ”€â”€ lmdb/\n",
    "â”‚   â”œâ”€â”€ train/\n",
    "â”‚   â””â”€â”€ test/\n",
    "â”œâ”€â”€ checkpoints/\n",
    "â”‚   â””â”€â”€ epoch_10.pt\n",
    "â””â”€â”€ results/\n",
    "    â”œâ”€â”€ zeroshot_metrics.json\n",
    "    â”œâ”€â”€ zeroshot_confusion_matrix.png\n",
    "    â”œâ”€â”€ linear_probe_metrics.json\n",
    "    â”œâ”€â”€ linear_probe_confusion_matrix.png\n",
    "    â”œâ”€â”€ linear_probe_classifier.pkl\n",
    "    â”œâ”€â”€ train_features.npy\n",
    "    â”œâ”€â”€ test_features.npy\n",
    "    â”œâ”€â”€ metrics_comparison.csv\n",
    "    â””â”€â”€ final_report.txt\n",
    "\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(final_report)\n",
    "\n",
    "# Save report\n",
    "with open(f\"{DRIVE_RESULTS}/final_report.txt\", 'w') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(f\"\\nâœ… Final report saved to {DRIVE_RESULTS}/final_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated artifacts\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ARTIFACTS SAVED TO GOOGLE DRIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def list_files_recursive(path, prefix=\"\"):\n",
    "    \"\"\"List files recursively with tree structure\"\"\"\n",
    "    items = sorted(os.listdir(path))\n",
    "    for i, item in enumerate(items):\n",
    "        item_path = os.path.join(path, item)\n",
    "        is_last = (i == len(items) - 1)\n",
    "        \n",
    "        connector = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "        \n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{prefix}{connector}{item}/\")\n",
    "            extension = \"    \" if is_last else \"â”‚   \"\n",
    "            list_files_recursive(item_path, prefix + extension)\n",
    "        else:\n",
    "            size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "            print(f\"{prefix}{connector}{item} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\n{DRIVE_BASE}/\")\n",
    "list_files_recursive(DRIVE_BASE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll artifacts saved to: {DRIVE_BASE}\")\n",
    "print(f\"\\nYou can now:\")\n",
    "print(f\"  1. Review metrics in {DRIVE_RESULTS}/final_report.txt\")\n",
    "print(f\"  2. Use the trained model: {DRIVE_CHECKPOINTS}/epoch_10.pt\")\n",
    "print(f\"  3. Download artifacts from Google Drive for local analysis\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}